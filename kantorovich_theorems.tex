\subsection{Two Theorems on fast evaluations of the Kantorovich Distance}
In this section, two theorems will be presented that will be crucial in the algorithms for stochastic search presented througtout this paper.
\subsubsection{Kantorovich Distance for optimal weights}
\label{sec:optimal-weights-proof}
In this section, we will prove a theorem that is of great practical significance for the remainder of this thesis. It is based on the ideas of \cite{Dupacova2003}, theorem 2. 

Consider two discrete stochastic processes represented by their index sets $I$ and $J$. The values of each scenario $\xi_i^t,\, i\in I$ and $\nu_j^t,\, j\in J$ at each timestep are known, as are the discrete probabilities $p_i$ of the process $I$. The probabilities of process $J$ are not given explicitly, but defined as the set of weights that minimize the Kantorovich Distance $D_K(I,J)$ between the stochastic processes:
\begin{equation}
  \label{eq:define-optimal-weights-q}
  q := \underset{q}{\operatorname{argmin}}\left\{\sum_{i\in I}\sum_{j\in J}\eta_{ij}c_{ij},\; \sum_{i\in I}\eta_{ij} = q_j,\; \sum_{j\in J}\eta_{ij} = p_i,\;\eta\geq 0,\; q\in \left[0,1\right]^{|J|} \right\}
\end{equation}
with $c_{ij} = c(\xi_i,\nu_j)$. While not fixing $q$ at first seems to make the problem more difficult by introducing more free variables, it actually makes it possible to state both the probability distribution $q$ and the value of the Kantorovich Distance $D_K$ in closed form. The following theorem and algorithm \ref{alg:optimal-weights} will show how this can be achieved.
\begin{thm}[Optimal Weights]
  \label{thm:optimal-weights}
  The optimal weights $q$ in eqn. \ref{eq:define-optimal-weights-q} are given by
  \begin{equation}
    \label{eq:optimal-weights-in-thm}
    q_j := \sum_{i\in I_j} p_i\;\text{ with } I_j:=\left\{i\in I| j = \underset{k\in J}{\operatorname{argmin}}\; c_{ik}\right\}.
  \end{equation}
  The value of $D_K$, defined as the minimum of the set in eqn. \ref{eq:define-optimal-weights-q} when fixing $q$ to the values given in eqn. \ref{eq:optimal-weights-in-thm}, takes on the value
  \begin{equation}
    \label{eq:define-Dk-optimalweights-thm}
    D_K = \sum_{i\in I}p_i\min\limits_{j\in J}c_{ij}
  \end{equation}
\end{thm}
Dupacova et al. proof a very similar theorem (\cite{Dupacova2003}, theorem 2), whith the difference that there, $J\subset I$. Their proof, which could be applied with only slight modifications, uses the primal and dual representations of the Kantorovich minimum flow problem. Instead, we will try to give the following, more intuitive
\begin{proof}
First, we will show the correctness of the representation of $D_K$ in eqn. \ref{eq:define-Dk-optimalweights-thm}. $D_K$ is defined as
\begin{align}
  D_K := \min\limits_{q, \eta}& \sum_{i\in I}\sum_{j\in J}\eta_{ij}c_{ij}\\
  \text{s.t.}&\sum_{i\in I}\eta_{ij} = q_j\label{eqn:eta-q-in-optimalweights-proof}\\
  &\sum_{j\in J}\eta_{ij} = p_i\label{eqn:eta-p-in-optimalweights-proof}\\
  &\eta \geq 0\\
  &0 \leq q \leq 1
\end{align}
The problem can be solved disregarding the variables $q_j$. The variables only appear in (\ref{eqn:eta-q-in-optimalweights-proof}), and their bounds are implicit in the remainig equations: $q_j\geq 0$ is implicit in $\eta_{ij}\geq 0$, and $q_j\leq 1$ is implicit in 
\[\sum_{j\in J}q_j=1\]
which is again implicit in (\ref{eqn:eta-p-in-optimalweights-proof}) as shown above in  (\ref{eq:proof-sum-q-redundant}). Therefore, $D_K$ can be computed using the reduced problem
\begin{align}
  D_K := \min\limits_{\eta}& \sum_{i\in I}\sum_{j\in J}\eta_{ij}c_{ij}\\
  \text{s.t.}&\sum_{j\in J}\eta_{ij} = p_i\\
  &\eta \geq 0
\end{align}
and $q$ can be deduced from the resulting $\eta$. In this reduced problem, the constraint Jacobian decomposes into independent blocks, with the variables $\eta_{ij}$ and $\eta_{kl}$ being independent, if $i\neq k$. This allows us to reformulate the problem as a sum of optimal values of independent optimization problems:
\begin{equation}
  \label{eqn:Dk-decomposition-in-mi}
  D_K = \min \sum_{i\in I} m_i
\end{equation}
with
\begin{align}
  \mathcal{D}_i\; :\; m_i :=\min\limits_{\eta^i}&\sum_{j\in J}\eta_{j}^ic_{j}^i\\
  \text{s.t.}&\sum_{ij}\eta_j^i = p_i\\
  &\eta_j^i\geq 0.
\end{align}
Each problem $\mathcal{D}_i$ is only has one constraint. The solution is trivial and can be easily deduced from the KKT conditions. The KKT conditions for a problem $\mathcal{D}_i$ are
\begin{equation}
  \label{eq:D-i-KKT-in-optimalweightsproof}
  \left[
  \begin{array}{c}
  c_j^i + e\lambda^i -\mu_j^i\\
  \sum_{j}\eta_j^i\\
  \eta_j^i\mu_j^i
  \end{array}
  \right]
  = \left[
    \begin{array}{c}
      0\\p_i\\0
    \end{array}
\right]
\end{equation}
with $\mu_j^i\geq 0$. As a linear problem which obviously satisfies LICQ, KKT conditions are sufficient for minimum. Since the values are computed as norms, we know that $c_j^i\geq 0$. With this information, the KKT conditions allows only for the following solution:
\begin{align}
  \label{eq:optimal-eta-optmimalweightsproof}
  \lambda^{\mathrm{opt}} &:= -\min\limits_{j\in J}c_j^i\\
  \mu_j^{i,\mathrm{opt}} &:= c_j^i - \min\limits_{j\in J}c_j^i\\
  \eta_j^{i,\mathrm{opt}} &:= \left\{ \begin{array}{lr}p_i&\text{if }j=\underset{j\in J}{\operatorname{argmin}}\, c_j^i\\0&\text{otherwise}\end{array}\right.
\end{align}
The KKT conditions for linear problems are sufficient conditions for optimality. Therefore, we have 
\begin{equation}
  m_i^{\mathrm{opt}} = \sum_{j\in J}\eta_j^{i,\mathrm{opt}}c_j^i = p_i\min\limits_{j\in J}c_j^i.
\end{equation}
Using this result in conjunction with (\ref{eqn:Dk-decomposition-in-mi}) yields the desired representation (\ref{eq:define-Dk-optimalweights-thm}) of $D_K$. The optimal solution postulated for $q$ (\ref{eq:optimal-weights-in-thm}) can be verified using the optimal weights $\eta_j^{i,\mathrm{opt}}$ in constraint (\ref{eqn:eta-q-in-optimalweights-proof}). This closes the proof.
\end{proof}
\begin{algorithm}
  \KwIn{Scenarios and probabilities for SP 1 $(\xi_i,p_i)\, i\in I$, scenarios $\nu_j,\, j\in J$ for SP 2}
  \KwOut{Optimal weights $q_j$ for the scenarios $j\in J$ and $D_K(I,J)$}
  $c_{ij} \leftarrow \Vert \xi_i - \nu_j\Vert$\tcc*{Any norm can be chosen here}
  $q_j \leftarrow 0$\;
  $D_K(I,J) \leftarrow 0$\;
  \ForEach{$i\in I$}{
    $k = \underset{j\in J}{\operatorname{argmin}}\{c_{ij}\}$\;
    $q_k\leftarrow q_k + p_i$\;
    $D_K(I,J) \leftarrow D_K(I,J) + c_{ik}$\;
  }
  \caption{Optimal weights}
  \label{alg:optimal-weights}
\end{algorithm}
\subsubsection{Kantorovich Distance and Clustering Algorithms}
In this section, a corrolary to theorem \ref{thm:optimal-weights} is proven, that will allow for a very efficient rephrasing of the problem defined in (\ref{eq:symbolic-optimization-with-minflow2}).
For this section, we will disregard the time aspect of stochastic processes and only consider random variables.
Suppose in (\ref{eq:symbolic-optimization-with-minflow2}), instead of a stochastic process we were given a set of samples from a random variable.
The assignment is to find a smaller set of samples of a given carinality that best fits the original distribution under the Kantorovich Distance.
This can be formalized as the following
\begin{problem}[Optimal Kantorovich Approximation (Continuous)]
\label{prb:CE-Kantorovich-randvar}
  Given a discrete random variable $\xi :\Omega_1\rightarrow \mathbb{R}^n$, $|\Omega_1|<\infty$, find the discrete random variable $\nu : \Omega_2\rightarrow\mathbb{R}^n$ with $|\Omega_2 | \leq K$ events that minimizes the Kantorovich Distance $D_K(\xi,\nu)$.
\end{problem}
A related problem is to find the optimal reduction of a random variable:
\begin{problem}[Optimal Kantorovich Approximation (Discrete)]
  \label{prb:DE-Kantorovich-randvar}
  Given a discrete random variable $\xi : \Omega_1\rightarrow \mathbb{R}^n$, $|\Omega_1|<\infty$, find the discrete random variable $\nu : \Omega_2\rightarrow\mathbb{R}^n$ with $|\Omega_2 | \leq K$ events that minimizes the Kantorovich Distance $D_K(\xi,\nu)$.
  In additition, it must hold that $\nu(\Omega_2)\subset\xi(\Omega_1)$.
\end{problem}
These problems are the direct transcription of the Kantorovich Distance problems for discrete random variables.
The difference between the two is that the random variable that is a solution to the second problem must map to values that actually appeare as outcomes of the first random variable.
While there is no such restriction on the solution of the first problem.

The goal of this section is to prove that these problems can be expressed in a much more well known form as the $K$-mean and $K$-medoid problems. We will introduce these two problems and discuss them.
\begin{problem}[$K$-Means]
  \label{prb:kmeans-problem-definition}
  Given a number $K\in \mathbb{N}$, a finite set of points $\{x_1,\ldots , x_N\} = X\subset\mathbb{R}^n$ and a metric $c:I\times \mathbb{R}^n\rightarrow\mathbb{R}_+$, find the set $M\subset\mathbb{R}^{n}$, with $|M|=K$, such that
  \begin{equation}
    \label{eq:kmeans-problem-definition}
    M = \underset{\hat{M}\subset\mathbb{R}^{n\cdot K}}{\operatorname{argmin}} \sum_{i=1}^N\min\limits_{m\in \hat{M}}c(x_i, m)
  \end{equation}
\end{problem}
\begin{problem}[$K$-Medoids]
  \label{prb:kmedoids-problem-definition}
  Given a number $K\in \mathbb{N}$, a finite set of points $\{x_1,\ldots , x_N\} = X\subset\mathbb{R}^n$ and a metric $c:I\times \mathbb{R}^n\rightarrow\mathbb{R}_+$, find the set $M\subset X$, with $|M|=K$, such that
  \begin{equation}
    \label{eq:kmedoids-problem-definition}
    M = \underset{\hat{M}\subset X}{\operatorname{argmin}} \sum_{i=1}^N\min\limits_{m\in \hat{M}}c(x_i, m)
  \end{equation} 
\end{problem}
The $K$-mean and $K$-medoid problems are two of the most extensively used formulations in machine learning and data mining. 
They are very useful for the unsupervised search of clusters in information. 

We are now prepared for the following
\begin{thm}[Equivalence of K-means and optimal Kantorovich Approximation]
  \label{thm:kmeans-kantorovich}
Consider a discrete random variable $\xi : \Omega_1\rightarrow \mathbb{R}^n$ and an integer $K$ as input to problem \ref{prb:CE-Kantorovich-randvar}.
If the set $M$ is a solution to problem \ref{prb:kmeans-problem-definition} with input set $X=\xi(\Omega_1)$, then the random variable $\nu : M \rightarrow M,\, m\mapsto m$ is a solution to problem \ref{prb:CE-Kantorovich-randvar} for the data above.
\end{thm}
\todo[inline]{This proof only works for 1/n probability distributions.}
\begin{proof}
  The proof is very easy given theorem \ref{thm:optimal-weights}. 
  We will derive sufficient conditions on the solution $\nu$ to problem \ref{prb:CE-Kantorovich-randvar} and show that the proposed solution $\nu$ meets these conditions.
  $\nu$ can be characterized by the following optimization problem:
  \begin{equation}
    \label{eq:characterize-nu-in-problemequivalence-proof}
    \min\limits_{\nu}D_K(\xi,\nu)
  \end{equation}
  The objective function $D_K$ is defined by the Kantorovich mass transportation problem
  \begin{align}
    \label{eq:kantoro-in-problemequivalence-proof}
    D_K = \min\limits_{\eta, q}& \sum_{i\in I}\sum_{j\in J}\eta_{ij}c(\xi_i,\nu_j)\\
    \text{s.t.}&\sum_{i\in I}\eta_{ij} = q_j\\
    &\sum_{j\in J}\eta_{ij} = \frac{1}{n_i}\\
    &0\leq \eta.
  \end{align}
  A solution trivially always exists, because the feasible set of the optimization problem is not empty, and the objective has a lower bound ($D_K\geq 0$).
  Note that the choice of the sets $\Omega_1$ and $\Omega_2$ is arbitrary since they do not appear in any form in the optimization problem.
  All that matters are the sets $\xi(\Omega_1)$ and $\nu(\Omega_2)$.
  From theorem \ref{thm:optimal-weights} we know that for any fixed values $\nu$, the solution to the inner problem can be directly computed using algorithm \ref{alg:optimal-weights}.
  For given $\nu$, the solution (see (\ref{eq:define-Dk-optimalweights-thm})) is
  \begin{equation}
    \label{eq:repeat-Dk-optimalweights-for-kmeansproof}
    D_K(\xi,\nu) = \sum_{i\in I}\frac{1}{n_i}\min\limits_{j\in J}c(\xi_i,\nu_j).
  \end{equation}
  Combining with (\ref{eq:characterize-nu-in-problemequivalence-proof}), $\nu$ is can finally be characterized by
  \begin{equation}
    \label{eq:end-of-kmeans-proof}
    \nu(\Omega_2) = \argmin \frac{1}{n_i}\sum_{i\in I}\min\limits_{j\in J} c(\xi_i,\nu_j) = \argmin \sum_{i\in I}\min\limits_{j\in J} c(\xi_i,\nu_j).
  \end{equation}
  The constant factor $\frac{1}{n_i}$ is applied to all elements of the set and therefore does not change the result.
  We now have characterized the set $\nu(\Omega_2)$ as a solution to problem \ref{prb:kmeans-problem-definition}.
  This closes the proof.
\end{proof}
The same relationship can be easily shown for problems \ref{prb:DE-Kantorovich-randvar} and \ref{prb:kmedoids-problem-definition}. As this proof follows the same idea as the one above, it will not be presented here.
\todo[inline]{State theorem on DE problem (no need to prove)}
\todo[inline]{maybe include standard algorithms for kmeans and kmedoids (PAM)}
\begin{comment}
In this section, we analyze the computation of the Kantorovich distance for a different setup. Consider again two discrete stochastic processes $I$ and $J$ over time stages $T$ with values $\xi_i^t,\,i\in I$ and $\nu_j,\, j\in J,\;t\in T$. Here, the values $\xi_i^t$ and their probabilities $p_i$ for process $I$ are fixed, while the values $\nu_j^t$ and probabilities $q_j$ for process $J$ are to be selected such that the Kantorovich Distance $D_K(I,J)$ is minimal. The problem can be expressed as the NLP
\begin{align}
  \min\limits_{\eta,q,\nu}&\sum_{i\in I}\sum_{j\in J}\eta_{ij}c(\xi_i,\nu_j)\\
  \text{s.t.}&\sum_{i\in I}\eta_{ij} = q_j\\
  &\sum_{j\in J}\eta_{ij} = p_i\\
  &\eta_{ij}\geq 0\\
  &q\geq 0
\end{align}
As shown above in theorem \ref{thm:optimal-weights}, for any given set of $\nu_j^t$ the variables $\eta_{ij}$ and $q$ will take on the values 
\begin{align}
  \eta_{ij} &= \left\{\begin{array}{lr}p_i&\text{if }j=\underset{j\in J}{\operatorname{argmin}}\, c(\xi_i,\nu_j)\\0&\text{otherwise}\end{array}\right. ,\\
  q_j &= \sum_{i\in I_j}p_i\text{ with } I_j= \left\{i\in I|j=\underset{k\in J}{\operatorname{argmin}}\, c(\xi_i,\nu_j)\right\}.
\end{align}
This corresponds to a \textit{set partitioning} of the set $I$ into $|J|$ parts, defined by a mapping 
\[P:I\rightarrow J,\; i\mapsto \underset{j\in J}{\operatorname{argmin}}\, c(\xi_i, \nu_j)\]
Given the distances $c(\xi_i,\nu_j)$, the optimal partitioning of $I$ can be computed using theorem \ref{thm:optimal-weights} and algorithm \ref{alg:optimal-weights}. 

Consider now the inverse situation. Given a set partitioning $P$, compute the optimal values of the stochastic process $\nu_j^t$. A fast and direct way of computing these values without having to solve the optimization problem, is given by this
\begin{thm}[optimal scenarios for fixed flows]
  Given a partitioning of a stochastic process $I$ and a convex metric $c$, the (not necessarily unique) stochastic process $J$ that minimizes the objective of the Kantorovich Distance $D_K(I,J)$ (note that this is not equal to the Kantorovich Distance - it is actually strictly greater) is defined by
  \begin{equation}
    \nu_j := \frac{1}{2}\left(\xi_i+\xi_k)\right)\text{ with }(i,k) = \underset{(r,l)\in I_j^2}{\operatorname{argmax}}\, c(\xi_r,\xi_l)
  \end{equation}
  with $I_j:=P^{-1}(j)$. Given this representation, the Kantorovich Distance can be computed using algorithm \ref{alg:optimal-weights}. 
\end{thm}
\begin{proof}
  The objective fut
\end{proof}
\begin{Note}
  This is very interesting: The Optimal Kantorovich Distance can be expressed as a clustering problem.
\end{Note}
\todo[inline]{Insert the proof for optimal tree fixed flow}
\todo[inline]{Insert algorithm object for how to compute the optimal tree for fixed flow according to the proof}
\end{comment}