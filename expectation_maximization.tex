\section{Expectation Maximization Algorithms}
\label{sec:expect-max-algos}
The previous section has established that the solution of the optimal random variable approximation can be deduced from the solution of a K-Means problem.
This section is organized as follows.
In section \ref{sec:k-means-standard}, the K-means algorithm is further explained.
Building on this analysis, an algorithm to generate optimal trees based on the K-Means algorithm is devised.
Promising results are reported in section \ref{sec:kmeans-results} for this new algorithm, both for speed as well as quality of the approximation.

In section \ref{sec:k-means-as-EM}, the relationship between the K-Means algorithm and Expectation Maximization algorithms is explained.
Building on this relationship, the discretization of the infinite dimensional Kantorovich distance introduced in section \ref{sec:math-foundations} is revisited.
It is found that the discretization exhibits a hidden prior/bias on the distribution.
\subsection{The K-Means Algorithm}
\label{sec:k-means-standard}
In this section, the K-Means algorithm briefly introduced in \ref{sec:kantorovich-and-clusters} will be discussed in detail.

The K-Means algorithm is usually thought of in the framework of unsupervised data mining.
Given a set of unlabeled data $X$, the goal is to find labels $Y$ for every $x\in X$, such that the data points with common labels meet some form of homogeneity criterion.
The space $U\supset X$ is generally endowed with some measure of dissimilarity $c: X\times U\rightarrow \mathbb{R}_+$.
The homogeneity criterion is expressed in terms of the dissimilarity between the elements of a cluster (that is all $x\in X$ that share the same label $y\in Y$) and the ``centroid'' $\mu\in U$ of the cluster.
The two most common choices for $U$ are $U=\mathbb{R}^n$ and  $U=X$.
These choices correspond to the K-Means and the K-Medoids algorithms from the previous section.
The general procedure is independent of this choice.

The algorithm demands the number of different labels $K=|Y|$ as an input parameter.
Binary assignment variables $z_{ij}$ are introduced, which are one, if label $y_j$ is attached to data point $x_i$, and zero otherwise.
Obviously,
\[\sum_{j\in Y}z_{ij} = 1\]
has to hold.
Using these labels, we can define the objective function (sometimes known as the ``distortion function'') for the K-Means problem as
\begin{equation}
  \label{eq:5}
  \mathcal{J}(z, \mu) = \sum_{i}\sum_{j}z_{ij}c(x_i, \mu_j).
\end{equation}
The K-Means problem can be expressed as
\begin{equation}
  \label{eq:3}
  \min\limits_{z, \mu}\mathcal{J}(z, \mu).
\end{equation}
This mixed integer optimization problem is in general intractable, due to the often large number of data points.
The problem, though hard to solve to global optimality, can be solved quite efficiently using coordinate descent.\footnote{Coordinate descent, also known as the alternating variables method, is the cyclic application of gradient descent to each dimension of the variable vector \cite{Nocedal1999}.}
Note that the optimization of $J$ with respect to each single variable is very easy.
The solution for the assignment variables is
\begin{equation}
  \label{eq:4}
  z_{ij} = \left\{\begin{array}{ll}1&\text{if }j=\underset{i}{\argmin}\; c(x_i,\mu_j)\\0&\text{otherwise} \end{array}\right. .
\end{equation}
For the centroids, the optimal value depends on the choice of $U$ and $c$.
For the case of $U=\mathbb{R}^n$, $c(x,\mu) = \Vert x-\mu\Vert^2$ it becomes the solution to a quadratic equation
\begin{equation}
  \label{eq:6}
  \mu_j = \left(\sum_{i}z_{ij}x_{ij}\right)\left / \left(\sum_{i}z_{ij}\right)\right. .
\end{equation}
For $U=X$ and an arbitrary dissimilarity measure $c$, there is no general analytic solution, so all distance pairs $c(x_i, x_j)$ have to be evaluated:
\begin{equation}
  \label{eq:7}
  \mu_j = \underset{\hat{\mu}\in X}{\argmin}\; \sum_{i}\sum_{j}z_{ij}c(x_i,\hat{\mu})
\end{equation}
This is generally not considered to be a significant drawback, since many strategies have been found to limit the number of evaluations in actual implementations (see \citeasnoun{Bishop2006} for details).

The algorithm is composed of alternating optimizations of J with respect to $z$ and $\mu$.
At every iteration, the objective function is guaranteed to decrease.
There is no guarantee that the value that the algorithm converges to is a global optimum.
However, If some care is taken at the initialization of the variables, in practice the algorithm generally yields nearly optimal values \cite{Arthur2006}.
The procedure is outlined below in algorithm \ref{alg:k-means}.
\begin{algorithm}
  \label{alg:k-means}
  \caption{K-Means/K-Medoids Expectation Maximization}
  \KwIn{Metric $c$, data points $X = \{x_1,\ldots , x_N\}$.}
  \KwOut{Cluster means $M=\{m_1,\ldots , m_K\}$, assignments $z_{ij}$ mapping $X$ to  $M$.}
  \lIf{K-Means}{$U = \mathbb{R}^n$\;}
  \lIf{K-Medoids}{$U = X$\;}

  (Randomly) initialize $m_i$\;
  \While{$z_{ij}$ change}{
    $z_{ij}\leftarrow \left\{\begin{array}{ll} 1 & \text{if }j = \underset{k}{\argmin}\; c(x_i, \mu_k)\\0&\text{otherwise}\end{array}\right.$\tcc*{Expectation step}
    $\mu_j\leftarrow \underset{\hat{\mu}\in U}{\argmin}\; \sum_{i}\sum_{j}z_{ij}c(x_i,\hat{\mu})$\tcc*{Maximization step}
  }
\end{algorithm}
\subsection{The K-Means Algorithm for Trees}
\label{sec:k-means-algorithm-trees}
In section \ref{sec:kantorovich-and-clusters}, we showed that constructing the optimal discrete random variable $\bs\chi$ that best approximates a given discrete uniformly distributed random variable $\bs\xi$ is equivalent to solving the K-Means problem.
This approach does not directly apply to stochastic processes.
In this section, the K-Means coordinate descent algorithm described in the previous section will be translated to the specific case of constructing discrete stochastic processes given a fixed filtration tree $\mathcal{T}$.
\subsubsection{The Algorithm}
\label{sec:k-means-for-trees-alg}
Consider a discrete stochastic process $\bs\xi(I, \mathcal{F}_1, p)$ with uniform distribution $p$.
The algorithm will operate only on the scenarios $\xi_i,\; i\in I$ of $\bs\xi$.
The aim of the algorithm is to find a discrete stochastic process $\bs\chi(J, \mathcal{F}_2,q)$ which satisfies a given filtration tree $\mathcal{T}$.
For ease of the derivation we will assume the dissimilarity measure $c$ to be the squared euclidean norm $\Vert\cdot\Vert_2^2$.
In analogy to \eqref{eq:5}, assignment variables $z_{ij}$ are used to link the scenarios $\xi_i$ to the scenarios $\chi_j$.
The difficulty of the current problem is, that in contrast to the random variable events, the events $\chi_j$ of the tree are not independent.
Instead, they are linked through the filtration tree to the node values $y$, because all nodes except for the leafs are shared among more than one scenario.
The objective function is
\begin{equation}
  \label{eq:8}
  \mathcal{J}(z, y) = \sum_{i\in I}\sum_{j\in J}z_{ij}c(\xi_i, \chi_j(y)).
\end{equation}
with $y$ the vector of  node values.
The problem will again be solved using a coordinate descent algorithm on the objective $\mathcal{J}$, alternating between iterations in $z$ and $y$.
The optimization with respect to the assignment variables $z$ is simple.
The optimal solution always selects the shortest path, given the placements $y$.
For fixed $y$, similar to \eqref{eq:4} we have
\begin{equation}
  \label{eq:10}
  z_{ij} = \left\{\begin{array}{ll}1&\text{if }j=\underset{k}{\argmin}\; c(\xi_i,\chi_k(y))\\0&\text{otherwise} \end{array}\right. .
\end{equation}
The optimization with respect to the node values $y$ seems at first glance more difficult, but will prove to be just as easy as with random variables.
We will first consider the problem for continuous-event trees.
As discussed above we choose $c(\xi_i, \chi_j) = \Vert \xi_i-\chi_j\Vert^2$ as the dissimilarity function.
\begin{equation}
  \label{eq:11}
  \min\limits_y \mathcal{J}(z,y) = \min\limits_y \sum_{i\in I}\sum_{j\in J}z_{ij}\Vert \xi_i - \chi_j(y)\Vert^2
\end{equation}
As we are dealing with an unconstrained, convex, differentiable optimization problem, the minimum is attained at the zero of the derivative.
For node $k$ this means
\begin{equation}
  \label{eq:12}
  \frac{\partial \mathcal{J}(z,y)}{\partial y_k} = \sum_{i\in I}\sum_{j\in J}z_{ij}\frac{\partial c(\xi_i, \chi_j)}{\partial y_k}\overset{!}{=} 0.
\end{equation}
The derivative can be expanded to
\begin{equation}
  \label{eq:13}
  \frac{\partial c(\xi_i, \chi_j)}{\partial y_k} = \frac{\partial c(\xi_i, \chi_j)}{\partial \chi_j^t}\frac{\partial \chi_j^t}{\partial y_k}.
\end{equation}
From \eqref{eq:71} we have
\begin{equation}
  \label{eq:14}
  \frac{\partial \chi_j^t}{\partial y_k} =
  \left\{
    \begin{array}{ll}
      1&\text{if } k = n(j,t)\\0&\text{otherwise}
    \end{array}
  \right. .
\end{equation}
Using the definition, we finally get
\begin{align}
  \label{eq:15}
  y_k = \left(\sum_{i\in I}\sum_{(j,t)\in n^{(-1)}(k)}z_{ij}\xi_i^t \right) \left / \left(\sum_{i\in I}\sum_{(j,t)\in n^{(-1)}(k)}z_{ij}\right)\right. .
\end{align}
The optimal value of a node is the average value of all scenarios that are associated with this node through the variables $z_{ij}$ at the time step of the node.
Note that this is almost the same as the optimal solution to the regular K-Means algorithm \eqref{eq:6}.
This completes the steps of the algorithm for continuous-event trees.

For discrete-event trees, as long as the dissimilarity function $c$ is chosen such that it can be expressed as
\begin{equation}
  \label{eq:16}
  c(\xi_i, \chi_j) = \sum_{t\in T}c_t(\xi_i^t,\chi_j^t),
\end{equation}
the variables $y$ can similarly be computed stage-wise as the optimal nodes of
\begin{equation}
  \label{eq:17}
  y_k = \underset{\{l\in I|\exists (j,t)\in n^{(-1)}(k):z_{lj}=1\} }{\argmin}\; \sum_{i\in I}\sum_{(j,t)\in n^{(-1)}(k)}z_{ij}c(\xi_i^t, \xi_l^t).
\end{equation}
This is the optimal value of a scenario selected among all the scenarios that were associated with this specific node.
Algorithm \ref{alg:kmeans-trees} summarizes the steps of the K-Means algorithm for trees.
\begin{algorithm}
  \caption{K-Means for Trees}
  \label{alg:kmeans-trees}
  \KwIn{discrete SP $\bs\xi$, filtration tree $\mathcal{T}$}
  \KwOut{discrete SP $\bs\chi$ with filtration tree $\mathcal{T}$ with $\bs\chi = \underset{\hat{\bs\chi}}\argmin D_K(\bs\xi, \hat{\bs\chi})$}
  \BlankLine
  \While{$z_{ij}$ change}{
    $z_{ij}\leftarrow \left\{\begin{array}{ll} 1 & \text{if }j = \underset{l}{\argmin}\; c(\xi_i, \chi_l(y))\\0&\text{otherwise}\end{array}\right.$\tcc*{Expectation step}
    $y_k\leftarrow \underset{\hat{y}\in \mathbb{R}^d}{\argmin}\; \sum_{i}\sum_{j}z_{ij}c(\xi_i,\chi_j(\hat{y}))$\tcc*{Maximization step}
  }
\end{algorithm}
\subsubsection{Results}
\label{sec:kmeans-results}
In this section we will present preliminary computational results.
The K-Medoids  algorithm was implemented in {\sc matlab}.
The algorithm was compared against the stage-wise MILP algorithm introduced in section \ref{sec:MILP-selection-problem}.
The MILP at each stage was solved with Gurobi 4.01.
Since this algorithm solves the problem to full optimality for a given set of scenarios, it is considered to be the benchmark.

Two different tests were conducted and the results for both algorithms compared.
Both tests followed the same general procedure outlined below.
\begin{enumerate}
\item A discrete SP $\bs\xi$ defined by a set $\{\xi_i,\, i\in I\}$ of cardinality $N$ was sampled from a stochastic process.
  The process used here was a geometric brownian motion based on $\mathcal{N}(0,0.5)$ with four time steps.
\item The tree was computed using both algorithms.
  The feasibility tree was defined by the number of branches per node $n_c$.
  The run time was measured using {\sc matlab}'s built-in timing utilities.
\item Ten new sets of scenarios were used to assess the quality of the computed trees.
  For each of these new sets, the Kantorovich distance to the tree was computed.
\end{enumerate}

The first test compared the relative error in the Kantorovich distance between the tree and the test sets for different feasibility tree structures.
Values for $n_c$ of three, five and seven were tested.
For this test, the number of scenarios was fixed to $N=1000$ for the MILP algorithm and $N=3000$ for the K-Medoids algorithm.
The MILP algorithm was given less scenarios simply because it could not handle more in a sensible amount of time.
Note that even though it had three times as many scenarios to handle, the running time of the K-Medoids algorithm was still by two orders of magnitude lower.
Figure \ref{fig:compare-milp-kmedoids} shows the results for both algorithms.
Even though there is no proof that the K-Medoids algorithm will find the optimal solution, it achieves the same level of accuracy as the MILP benchmark algorithm.
The K-Medoids algorithm is able to outperform the benchmark algorithm, which is supposed to be a lower bound.
The superior quality of the solution found by the K-Means algorithm is due to its fast running time.
Because of that, the K-Medoids algorithm is able to handle a larger number of scenarios.
A larger number of scenarios corresponds to a better quality of the input data, of which the scenario set is only a discretization.
Consider again \eqref{eq:triangle-montecarlo-kantoro}:
\[\underbrace{D_K(\bs\chi,\hat{\bs\xi})}_{\text{Full Error}} \leq  \underbrace{D_K(\bs\chi, \bs\xi)}_{\text{algorithm error}} + \overbrace{D_K(\bs\xi,\hat{\bs\xi})}^{\mathclap{\text{Discretization error}}}\leq \underbrace{D_K(\bs\chi,\bs\xi)}_{\text{algorithm error}}+ \overbrace{\epsilon}^{\mathclap{\text{Discretization error}}}
\]
Here, $\hat{\bs\xi}$ is the original stochastic process, $\bs\xi$ is the sampled set of scenarios, and $\bs\chi$ is the scenario tree generated by the algorithm.
The discretization error $\epsilon$ is determined mainly by the number of scenarios that were sampled from $\hat{\bs\xi}$.
Therefore, the full error $D_K(\bs\chi,\hat{\bs\xi})$, which is plotted in figure \ref{fig:compare-milp-kmedoids}, can be smaller for a suboptimal method like the K-Medoids algorithm than the optimal solution, if the number of scenarios that were used while running the suboptimal method was larger.

The second test compared running times of both algorithms for different numbers $N$ of input scenarios.
The running times are depicted in the second plot of figure \ref{fig:compare-milp-kmedoids}.
The results show that the new K-Medoids algorithm outperforms the stage-wise MILP algorithm by a large margin.
In addition, it is capable of processing much larger sets of scenarios.
This is extremely important as the number of dimensions of the stochastic process increases.
\input{comparison_milp_kmedoids}
\subsection{The Expectation Maximization Algorithm}
\label{sec:k-means-as-EM}
In this section, the Expectation Maximization (EM) algorithm due to \citeasnoun{Dempster1977} will be introduced.
It is organized as follows.
First, we will present an argument how the K-Means algorithm of the previous section can be extended to compute an optimal function space approximation to the data with respect to the Kantorovich distance.
Then the Expectation Maximization algorithm will be introduced, and its most common implementation, the mixture of Gaussians model, will be presented.

The notation in this section is based on \citeasnoun{Bishop2006}.
Note that this notation, coming from the branch of machine learning, may differ in several ways from that used in other fields of mathematics.
\subsubsection{A Continuous Approach for the Kantorovich Distance}
\label{sec:cont-appr-kant}
Consider the K-Means algorithm of the previous section.
There, each data-point was assigned uniquely to one of the K data clusters.
These hard assignments stem from the fact that the algorithm operates on the discretized sample $\{\xi_n,\, n\in I\}$ of the original stochastic process.\footnote{For this section, we assume $\xi_n\in\mathbb{R}$.}
In a probabilistic context such as the scenario tree generation, these hard assignments are difficult to justify.
Instead of a discretization in terms of a discrete probability distribution (samples), we will analyze the approximation in a parametric function space.
A natural parametric function space that has favorable analytic properties are sums of Gaussian distributions.
Given a set of samples $\{\xi_n,\, n\in I\}$ with cardinality $|I|=N$, we assume that a probabilistic model of the form
\begin{equation}
  \label{eq:31}
  \mathbb{P}(x) = \sum_{n=1}^N\frac{1}{N}\mathcal{N}(x|\xi_n,\sigma_n)
\end{equation}
Can explain the data.
The goal is to find a random variable $\bs\chi(J,\mathfrak{A},\mathbb{Q})$ with a more compact representation.
Since we assume that the data form a weighted sum of Gaussians, the approximating random variable will have the same underlying model
\begin{equation}
  \label{eq:32}
  \mathbb{Q}(x) = \sum_{k=1}^K\pi_k\mathcal{N}(x|\mu_k, \sigma_k)
\end{equation}
with the means and variances $\mu_k$, $\sigma_k$ of each Gaussian, and the mixture coefficients $\pi_k$ that determine the relative contribution of each of the Gaussians to the model.
The Kantorovich distance of the distributions $\mathbb{P}$ and $\mathbb{Q}$ is
\begin{equation}
  \label{eq:33}
  D_K(\mathbb{P,Q}) = \min\limits_{\eta}\int\limits_{-\infty}^{\infty}\int\limits_{-\infty}^{\infty}(x-y)^2\eta(x,y)dxdy,\; \int\limits_{-\infty}^{\infty}\eta(x,y)dx = \mathbb{Q}(y),\;\int\limits_{-\infty}^{\infty}\eta(x,y)dy = \mathbb{P}(x)
\end{equation}
For the given Gaussian functions, for one cluster $k$ (with fixed parameters) and one sample $n$, the Kantorovich metric can be explicitly computed as
\begin{equation}
  \label{eq:34}
  D(\mathbb{p}_n,\mathbb{q}_k) = \int\limits_{-\infty}^{\infty}\left(x-\left(\sqrt{\frac{\sigma_n}{\sigma_k}}(x-\mu_k)+\xi_n\right)\right)^2\mathcal{N}(x|\mu_k,\sigma_k)dx.
\end{equation}
See figure \ref{fig:kantoro_gauss_explain} for an illustration how the above formula relates to the distributions.
\input{kantoro_gauss_eta_explanation}
This only works for one sample and one cluster distribution.
For the Kantorovich distance between all samples and all clusters, we need to define additional variables $\gamma_{nk}$ for every pair of samples $n$ and clusters $k$.
These variables correspond to the assignment variables $z_{nk}$.
However, in this model the assignments are not hard as they were for the K-Means problem.
Instead, a sample may be assigned to several clusters with different weight.
The $\gamma_{nk}$ are called \textit{responsibilities}.
They satisfy
\begin{equation}
  \label{eq:35}
  \sum_{k=1}^K\gamma_{nk} = 1
\end{equation}
which means that if all clusters $k$ are taken into account, the sample has to be fully accounted for.
In addition, we define
\begin{equation}
  \label{eq:36}
  N_k := \sum_{n=1}^N \gamma_{nk}.
\end{equation}
This number represents the total number of node-equivalents, for which cluster $k$ is responsible.
The value of $N_k/N$ represents the fraction of the data set that a specific cluster is responsible for.
We will later see that this value corresponds to the mixture parameter $\pi_k$.

The responsibilities make it possible to decompose the Kantorovich distance into the sub-parts that only involve one sample and one cluster.
The full Kantorovich distance can be expressed as the sum of the terms \eqref{eq:34}, weighted by the corresponding responsibilities:
\begin{equation}
  \label{eq:37}
  D_K(\mathbb{P,Q}) = \sum_{n=1}^N\sum_{k=1}^K\gamma_{nk}\int_\mathbb{R}\left(x-\left(\sqrt{\frac{\sigma_n}{\sigma_k}}(x-\mu_k)+\xi_n\right)\right)^2\pi_k\mathcal{N}(x|\mu_k,\sigma_k)dx
\end{equation}
The goal is to find the function $\mathbb{Q}(x)$ that minimizes \eqref{eq:37}.
$\mathbb{Q}$ is parametrized by the mean values $\mu_k$, the variances $\sigma_k$, the mixing coefficients $\pi_k$, and the responsibilities $\gamma_{nk}$:
\begin{equation}
  \label{eq:38}
  \min\limits_{\mu_k,\sigma_k,\pi_k, \gamma_{nk}}D_K(\mathbb{P,Q}(\mu_k,\sigma_k,\pi_k, \gamma_{nk}))
\end{equation}
Just as for K-Means, the optimization problem is hard to solve simultaneously.
If, however, either the responsibilities $\gamma_{nk}$ or the distribution parameters $(\mu_k, \sigma_k, \pi_k)$ are fixed, the optimization becomes easy.
This property, which resembles the structure of K-Means, facilitates the use of coordinate descent.
In the following, we will derive the update formulas for the parameters.
We will always fix all but one parameter.

First, we will consider the mean values.
Differentiating \eqref{eq:37} with respect to $\mu_k$ yields
%\begin{equation}
\begin{multline}
  \label{eq:39}
  \frac{\partial }{\partial\mu_k}D(\mathbb{P,Q}) = \sum_{n=1}^N\gamma_{nk}\frac{1}{\sqrt{2\pi}}\int_\mathbb{R}\exp\left(-\frac{(x-\mu_k)^2}{2\sigma_k}\right)\left(\sqrt{\frac{\sigma_n}{\sigma_k}}(\mu_k-x)-\xi_n+x\right)\cdots\\ \left((x-\mu_1)\left(\sqrt{\frac{\sigma_n}{\sigma_k}}(\mu_k-x)-\xi_n+x\right)+2\sqrt{\sigma_k\sigma_n}\right)dx \overset{!}{=} 0
\end{multline}
%\end{equation}
Evaluating the integral  yields
\begin{align}
  \label{eq:40}
 0 &=\sum_{n=1}^N\gamma_{nk}\left[(\xi_n-\mu_k)erf\left(\frac{\mu_k-x}{\sqrt{2\sigma_k}}\right) + \exp\left(-\frac{(x-\mu_k)^2}{2\sigma_k}\right)\left(\ldots\right)\right]_{-\infty}^\infty\\
 &=\sum_{n=1}^N\gamma_{nk}(\mu_k-\xi_n)
\end{align}
and finally solving for the parameter $\mu_k$, using the definition \eqref{eq:36}
\begin{equation}
  \label{eq:41}
  \mu_k = \frac{1}{N_k}\sum_{n=1}^N\gamma_{nk}\xi_n.
\end{equation}
The optimal mean value of cluster $k$ for a given set of responsibility variables is the average over all data points it is responsible for.
Note that this is identical to the update for the K-Means centroids, except that the hard assignments $z_{nk}$ are replaced by the soft assignments $\gamma_{nk}$.

The derivation of the optimal cluster variation follows a similar line.
After differentiating with respect to $\sigma_k$ and integrating, and solving one gets
\begin{equation}
  \label{eq:42}
  \sigma_k = \frac{1}{N_k}\sum_{n=1}^N \gamma_{nk}\sigma_n
\end{equation}
This expression cannot be computed yet, since we did not specify the variances $\sigma_n$ of the samples.
One simple choice would be to postulate the value of $\sigma_n$ to a fixed value independent of $n$.
The K-Means approach can be interpreted as using such a fixed, common variance, with the subtle choice of $\sigma_n\rightarrow 0$.
from \eqref{eq:42}, we have that the variance of the clusters would in that case be identical to the variance of the data.
Basically, the variance of the clusters is fixed beforehand.
We would, however, like to infer the variance of the data to make the approximation better.
Therefore, we use the clusters' mean values computed before to estimate the variance of the data points:
\begin{equation}
  \label{eq:43}
  \sigma_{nk} := (\mu_k-\xi_n)^2
\end{equation}
The cluster centroid is regarded as a single sample from the distribution $\mathcal{N}(x|\xi_n, \sigma_{nk})$.
From this one data point, the variance $\sigma_{nk}$ is inferred.
Using this definition, the update for $\sigma_k$ is
\begin{equation}
  \label{eq:44}
  \sigma_k = \frac{1}{N_k}\sum_{n=1}^N (\mu_k-\xi_n)^2
\end{equation}
This definition might not seems immediately obvious.
In the sections below, further evidence will be discovered that this update is, in fact, correct.
%
%The update of the responsibilities

In the following sections we will further discuss continuous approaches to approximating stochastic processes in tree structures.
We will introduce the Expectation Maximization algorithm and find out that the algorithm derived above is only an example of a larger group of algorithms that will become a powerful tool for scenario tree generation.
\subsubsection{The Expectation Maximization Algorithm}
The aim of this discussion is to give a short overview of the concept of EM.
This will enable us to analyze the discretization method used to discretize the Kantorovich distance in section \ref{sec:kantoro}.
See \cite{Bishop2006}, section 9 for a detailed treatment of the subject.

Consider the problem of fitting a model to a given data set.
Let the model parameters be denoted by $\theta$ and the data set by $X$.
In a probabilistic framework, fitting the model parameters $\theta$ to the data $X$ means maximizing the log-likelihood of the data given the model
\begin{equation}
  \label{eq:18}
  \max\limits_\theta \ln p(X|\theta)
\end{equation}
Let the model to generate the probability $p(X|\theta)$ be depending on a set of hidden variables $Z$.
In fact, we only have a model to generate the joint probabilities $p(X,Z|\theta)$ for any given $\theta$.
Using the sum rule we can express \eqref{eq:18} in terms of these probabilities:
\begin{equation}
  \label{eq:19}
  \ln p(X|\theta) = \ln\left(\sum_Zp(X,Z|\theta)\right)
\end{equation}
In the context of the tree generation, the parameters $\theta$ are the node values, the data set $X$ is the set of original scenarios generated from the original distribution, and the hidden variables $Z$ are the associations between the scenarios of $X$ and those of the tree, denoted by $z_{ij}$ in the previous sections.
These hidden variables encode the information which data scenario in $X$ was ``generated'' from which tree scenario.

The EM algorithm is a generic way to maximize the log-likelihood \eqref{eq:18} in this context.
The basic idea is very simple.
In each iteration $k$, the EM algorithm does two things.
First, in the \textbf{Expectation Step}, it updates the hidden variables $Z$ by estimating the distribution
\begin{equation}
  \label{eq:20}
  p(Z|X,\theta^k).
\end{equation}
In the second step, the \textbf{Maximization Step}, the parameters $\theta^k$ are optimized, such that they maximize the log-likelihood using the current state of the hidden variables.
The formula is
\begin{equation}
  \label{eq:21}
  \theta^{k+1} = \underset{\theta}{\operatorname{argmax}}\; \sum_Zp(Z|X,\theta^k)\ln p(X,Z|\theta)
\end{equation}
Algorithm \ref{alg:em} summarizes the procedure.
\begin{algorithm}
  \KwIn{Data $X$}
  \KwOut{Optimal parameters $\theta$}
  Initialize $\theta^0$ (randomly)\;
  $k\leftarrow 0$\;
  \While{Not Converged}{
    $\gamma^k(Z)\leftarrow p(Z|X,\theta^k)$\tcc*{Expectation Step}
    $\theta^{k+1}\leftarrow \underset{\theta}{\operatorname{argmax}}\; \sum_Z \gamma^k(Z)\ln p(X,Z|\theta)$\tcc*{Maximization Step}
  }
  \caption{Expectation Maximization Algorithm}
  \label{alg:em}
\end{algorithm}
%
\subsubsection{Mixture of Gaussian EM}
\label{sec:mixture-gaussian-em}
The model most commonly used in the EM algorithm is the Mixtures of Gaussians model.
The probability distribution of the data is approximated by a linear combination of Gaussian functions with means $\mu_j$ and covariance matrices $\sigma_j$.
The stochastic model can be expressed as
\begin{equation}
  \label{eq:22}
  p(x) = \sum_{j}\pi_j\mathcal{N}(x|\mu_j,\sigma_j).
\end{equation}
where $\pi_j$ are weights on the distributions and $\mathcal{N}$ is the Gaussian distribution
\begin{equation}
  \label{eq:25}
  \mathcal{N}(x|\mu,\sigma) = \frac{1}{\sqrt{2\pi\sigma}}\exp\left(-\frac{(x-\mu)^2}{2\sigma}\right)
\end{equation}
For the model to be a probability distribution,
\begin{equation}
  \label{eq:27}
  \sum_j\pi_j = 1
\end{equation}
has to hold.
The hidden variables again $z_{ik}$ describe the  assignment of data $x_i$ to clusters $k$.
In contrast to the K-Means model, the assignments are not fixed to one and zero, but instead are represented by the probability that data point $x_i$ is assigned to cluster $k$.
The posterior probabilities of $z_{ik}$ given the data and the parameters that describe the model are
\begin{equation}
  \label{eq:26}
  p(z_{nk}|x, \theta) = \frac{\pi_k\mathcal{N}(x_n|\mu_k,\sigma_k)}{\sum_j\pi_j\mathcal{N}(x_n|\mu_j, \sigma_j)}
\end{equation}
This expression is evaluated during the expectation phase of the algorithm.
The update step for the parameters is defined by the optimization problem \eqref{eq:21}.
Note that for this model, this is a constrained optimization problem, since \eqref{eq:27} has to hold.
The derivation is detailed in \citeasnoun{Bishop2006}.
The solution will be stated here for completeness and to show the similarity to the solution for tree structures stated below.
\begin{align}
  \label{eq:29}
  \mu_k^{t+1} &= \frac{\sum_np(z_{nk}|x, \mu, \sigma, \pi)\cdot x_n}{\sum_np(z_{nk}|x, \mu, \sigma, \pi)}\\
  \sigma_k^{t+1}&= \frac{\sum_np(z_{nk}|x, \mu, \sigma, \pi)\cdot (x_n-\mu_k^{t+1})^2}{\sum_np(z_{nk}|x, \mu, \sigma, \pi)}\\
  \pi_k^{t+1}&= \frac{1}{N}\sum_{n}p(z_{nk}|x, \mu, \sigma, \pi)
\end{align}

The K-Means algorithm can be regarded as the following limit to a Mixture of Gaussians model with covariance going to zero:
\begin{equation}
  \label{eq:23}
  \lim\limits_{\epsilon\rightarrow 0}\sum_j\pi_j\mathcal{N}(x| \mu_j,\epsilon I)
\end{equation}

The expectation step in the K-Means algorithm is the computation of the assignments $z_{ij}$.
Instead of probabilities $p(z_{ik}|x,\theta)$, the K-Means algorithm always assigns binary values one or zero to the hidden variables (``hard assignments'').

In contrast to K-Means, for the Mixture of Gaussians model the data points are not assigned ``hard'' to one of the clusters, but instead a ``responsibility'' $\gamma$ of each cluster to each data point is used to describe this relation.
This way, clusters may share points that are located in between them.
\subsection{Revisiting the Discretization}
\label{sec:revisiting-discretization}
In this section, we will revisit the discretization decision of the beginning.
We will find that the discretization used in the literature exhibits a certain bias with respect to the function space of the approximation.
The following discussion applies only to continuous-event trees.

Consider the following example.
A random variable $\hat{\bs\xi}(\mathbb{R}, \mathfrak{A}, \mathbb{P})$ with a probability distribution
\begin{equation}
  \label{eq:24}
  \mathbb{P}(x) = \frac{2}{3}\mathcal{N}(2,1) + \frac{1}{3}\mathcal{N}(-1,1)
\end{equation}
(see left plot of figure \ref{fig:rev-disc-xiorig}) is to be approximated by a discrete object.
Following the discretization procedure, a set of 45 events is sampled from the distribution.
The notion of the original distribution is discarded, and the Kantorovich distance is minimized for a given number of representative events, which shall be five in the example.
The top right plot shows the 5 mean values as blue dots on the x-axis.
Note that in the K-Means formulation, all points that are closest to a specific mean will be associated with this mean.
The resulting square intervals of influence are plotted with the height according to their probabilities.

These previous observations lead to following, important observation.
Using the discretized Kantorovich distance is equivalent to using a fixed number of piecewise constant functions as the basis functions for the approximation of the original probability distribution.
It is well known that the choice of basis functions effectively is the same as a prior distribution on the probability distribution that we expect to be underlying the observed events.
The central limit theorem suggests that Gaussian distributions are the best guess for a prior given no additional information.
Therefore, the common choice of discretization in the literature \cite{Dupacova2003} is not optimal and should be revised to be the Mixture of Gaussians - EM algorithm.
\input{discretization_animation}
\subsection{EM for Trees}
\label{sec:mixt-gauss-trees}
In this section, the EM algorithm is adapted to tree structures.
In section \ref{sec:k-means-algorithm-trees}, the K-Means algorithm was adapted to the tree generation problem.
As the K-Means algorithm is an instance of the class of EM algorithms, the tree generation procedure derived in this section follows the same steps as algorithm \ref{alg:kmeans-trees}.

Consider a set of scenarios $\{\xi_i,\, i\in I\}$ sampled from a stochastic process $\hat{\bs\xi}$.
The objective of the EM algorithm for trees is to find a scenario tree $\bs\chi$ whose node values, interpreted as parameters of a probabilistic model $\mathcal{D}$, `explain' the data scenarios $\xi_i$.
The hidden variables of the EM algorithm will be the binary assignment variables $z_{ij}$ that assign a sampled scenario $i\in I$ to a scenario $j\in J$ of the tree.

The general model structure under consideration is a parametric probability distribution $\mathcal{D}(\xi,z|\theta)$ with parameters $\theta$ representing the node values of $\bs\chi$.
The parameters of the model must be uniquely associated with the time steps of $\bs\xi$.
An example of this property are mean values.
Each scenario $j\in J$ of the tree is equipped with a mixing parameter $\pi_j$.
The model for the probability of a scenario $\xi_i$ given $\theta$ is
\begin{equation}
  \label{eq:28}
  p(\xi_i|\theta) = \sum_{j\in J}\pi_j\mathcal{D}(\xi_i|\theta_j),
\end{equation}
where $\theta_j$ is the parameter vector of a scenario.
$\mathcal{D}$ is a probability distribution.
Therefore, for \eqref{eq:28} to be a probability distribution,
\begin{equation}
  \label{eq:30}
  \sum_{j\in J}\pi_j = 1
\end{equation}
has to hold.
The tree information is preserved through the components of $\theta$.
Since each parameter is associated with a time step $t$ of the vectors $\xi_i$, we demand that $\theta_{jt}=\theta_{kt}$ for scenarios $j,k\in J$, if $n(j,t)=n(k,t)$.
Figure \ref{fig:em-tree} illustrates the tree structure for a Gaussian mixtures model.
The scenarios for the tree in the figure are
\[
\left\{\hansvec{\mu_1\\\mu_2\\\mu_4},\,\hansvec{\mu_1\\\mu_2\\\mu_5},\,\hansvec{\mu_1\\\mu_3\\\mu_6},\,\hansvec{\mu_1\\\mu_3\\\mu_7}\right\}.
\]
Each mean value $\mu_n$ (and its corresponding variance) can be uniquely identified with a specific node of the tree.
This makes it possible to enforce the tree structure.
Note that for Gaussian mixture models, no covariance information for different time steps can be considered.
\input{em_tree}
Using these definitions we can construct the EM algorithm for trees.
For illustration purposes, we will use the mixture of Gaussians model.

The first step of the algorithm is the initialization of the parameters $\mu_n$, $\sigma_n$ and $\pi_n$.
The expectation maximization algorithm will in general prove a considerably larger computational burden than the K-Means/K-Medoids algorithm described in section \ref{sec:k-means-algorithm-trees}.
Therefore, a possible initialization strategy is to run the K-Means algorithm on the same samples and infer the initial parameters of the EM-algorithm from the mean, variance and relative responsibilities of the clusters.

In the \textbf{expectation step}, the probabilities $p(z_{nk}|\xi, \mu,\sigma,\pi)$ are computed.
For this step, the scenarios $\xi_i$ of the sample set are regarded as data vectors.
The means $\mu_n$ and variances $\sigma_n$ of the nodes are assembled as scenarios $\mu_j$, $\sigma_j$ of the tree as depicted above.
For the variances this means concatenating the single matrices in block form
\begin{equation}
  \label{eq:58}
  \sigma_j = \left[\begin{array}{ccc}\sigma_{n(j,1)}&&0\\ &\ddots\\ 0&&\sigma_{n(j,T)}\end{array} \right].
\end{equation}
These scenarios $\{\xi_i,\, i\in I\}$ represent the data vectors, and $\{\nu_j,\, j\in J\}$ the mean values.
The responsibilities $z_{ij}$ are computed as though these scenario sets were the data and means of a multi-dimensional mixture of Gaussians algorithm:
\begin{equation}
  \label{eq:57}
  \gamma_{ij} = p(z_{ij}|\xi, \mu,\sigma,\pi) = \frac{\pi_j\mathcal{N}(\xi_i|\mu_j, \sigma_j)}{\sum_k\pi_k\mathcal{N}(\xi_i|\mu_k,\sigma_k)}
\end{equation}
The \textbf{maximization step} updates the parameters $\mu_n$, $\pi_j$ and $\sigma_n$ for a fixed set of $z_{ij}$.
Formally, this update is defined by
\begin{equation}
  \label{eq:59}
  \theta^{\mathrm{new}} = \underset{\theta}{\argmax}\; \sum_{i\in I}\sum_{j\in J}\gamma_{ij}\ln\,p(\xi_i,z_{ij}|\mu, \sigma)
\end{equation}
Here, the fact that the parameters of each stage, given $z_{ij}$, are independent, which was mentioned as a requirement, is crucial.
Just as in the derivation of the K-Means algorithm for trees, the update of the parameters depends solely on the scenarios and the time step that are associated with a particular node.
The derivation follows almost exactly the steps of section \ref{sec:k-means-for-trees-alg}.
The update formulas are
\begin{enumerate}
\item for the means
  \begin{equation}
    \label{eq:61}
    \mu_n = \frac{1}{N_c}\sum_{i\in I}\sum_{(j,t)\in n^{-1}(n)} \gamma_{ij}\xi_i^t
  \end{equation}
\item for the variances
  \begin{equation}
    \label{eq:62}
    \sigma_n = \frac{1}{N_c}\sum_{i\in I}\sum_{(j,t)\in n^{-1}(n)} \gamma_{ij}(\mu_n-\xi_i^t)(\mu_n-\xi_i^t)^{tr}
  \end{equation}
  \item for the mixing coefficients
    \begin{equation}
      \label{eq:63}
      \pi_j = \frac{N_c}{N}
    \end{equation}
\end{enumerate}
This shows that for a probabilistic model that satisfies certain independence criteria, the derivation of tree generation algorithm built on the idea of generative models is straightforward.
Depending on prior knowledge about the structure of the probability distribution of the stochastic process, much more accurate models can be built to approximate this process with less data points.
For the solution of the stochastic program, only the mean values of the scenarios are used.
The next section will show how the variance information of the EM-generated tree can be used to estimate the variance of the optimal value of the stochastic program.
\subsection{Variance Estimation from EM-Generated Trees}
\label{sec:variance-estimation}
In this section, it will be illustrated how the distribution generated with the mixture of Gaussians EM-algorithm can be used to estimate the variance of the optimal objective function with respect to the tree distribution.
This approximation assists in evaluating the quality and robustness of the solution.
The approximation below is possible, because EM algorithms, specifically the mixture of Gaussian algorithm that will be used here for illustration, preserve the information about the variance relative to each scenario (see figure \ref{fig:variance-estimation}).

Consider the stochastic optimization problem
\begin{subequations}
  \begin{align}
    \label{eq:49}
    \min&\;\int \hat{f}(x(\omega),\hat{\xi}(\omega))d\mathbb{P}(\omega)\\
    \text{s.t.}&\; \hat{h}(x(\omega),\hat{\xi}(\omega)) = 0\\
    &x(\omega)\geq 0
  \end{align}
\end{subequations}
with the stochastic process $\hat{\bs\xi}(\Omega, \mathcal{F}, \mathbb{P})$ representing the uncertain data.
The discretized form is\begin{subequations}\label{eq:46} \begin{align}
     \min\limits_{x}&\; f(x,\nu)\\
    \text{s.t.}&\; h(x,\nu) = 0\\
    &\; x \geq 0
  \end{align}
\end{subequations}
where $\bs\chi(J, \mathcal{T}, p)$ is the discrete stochastic process with corresponding scenario tree $\bs\chi(\mathcal{T}, \nu, p)$ whose node values $\nu$ are the mean values computed with the mixture-of-Gaussians-algorithm for trees.
Let $x^*=x^*(\nu)$ denote the solution to \eqref{eq:46} as a function of the node values.
Using a NLP solver, $x^*(\nu^0)$ for a fixed set of tree nodes can be computed.
The parametric sensitivity of optimal value $f(x^*,\nu^0)$ with respect to $\nu$ is
\begin{equation}
  \label{eq:47}
  \frac{\partial }{\partial \nu}f^{\mathrm{opt}}(x^*,\nu^0) =  \frac{\partial }{\partial \nu}\mathcal{L}(x^*,\lambda^*, \mu^*,\nu^0)
\end{equation}
with $\mathcal{L}$ the Lagrangean of the above optimization problem (\citeasnoun{Jongen2004}, theorem 3.2.3).
The objective function for stochastic programming problems can be expressed as
\begin{equation}
  \label{eq:45}
  f(x,\nu) = \sum_{j\in J} \pi_jf^s(x,\chi_j(\nu)).
\end{equation}
where $f^s$ is the objective function for a scenario.
It is the same for all scenarios of the tree -- the difference in scenarios is expressed through the scenarios $\chi_j(\nu)$, which depend on the node values through the filtration tree $\mathcal{T}$.
The weights $\pi_j$ are the mixing parameters of the EM algorithm.
Assume furthermore, that the objective can be decomposed by time steps:
\begin{equation}
  \label{eq:48}
  f(x,\nu) = \sum_{t=1}^T\sum_{j\in J}\pi_jf_{t}^s(x,\chi_{j}^t(\nu))
\end{equation}
For a tree generated with an EM algorithm, the set of scenarios $J$ is identical to the set of clusters $\{1,\ldots,K\}$.
\input{variance_estimation}

The tree nodes $\nu$ were generated using the Mixture of Gaussians EM-algorithm of the previous section.
Therefore, for each node value $\nu_n,\, n\in\mathcal{N}$, which represents the mean value of a Gaussian, a variance $\sigma_n$ is known from the result of the algorithm.
The discretized optimization problem \eqref{eq:46} is solved for the discrete distribution made up of the mean values of the tree node Gaussians.
For the solution $x^*$, the variance information is not taken into account.
Note that for fixed node values $\nu$, the scenario tree $\bs\chi(\mathcal{T},\nu,p)$ is a discrete random variable.
With the variances $\sigma$, the result of the EM-algorithm can be interpreted as a \textbf{distribution over discrete random variables}.
With this interpretation, we can compute the variance of the optimal objective value with respect to the probability distribution of the tree.
This value assists in identifying whether the tree structure is sufficiently `dense' to achieve the desired accuracy in the solution of the stochastic optimization problem.

The variance of the objective value with respect to the node values $\nu$ is defined as the Kantorovich distance between the distribution of scenarios optimal values over scenarios and the optimal solution
\begin{equation}
  \label{eq:50}
  \mathrm{Var}\left[f^{\mathrm{opt}}\right] :=  \sum_{k=1}^K\int_{\mathbb{R}^T} \left(f^{s,\mathrm{opt}}(\omega)-f^{s,\mathrm{opt}}\of{\chi_k(\nu)}\right)^2\mathbb{P}_k(\omega)d\omega
\end{equation}
The probabilities of the scenarios are given by the model
\begin{equation}
  \label{eq:55}
  \mathbb{P}_k(\omega) = \pi_k\mathcal{N}(\omega|\chi_{k}(\nu),\sigma_k).
\end{equation}
It is assumed that the variances $\sigma_k$ are sufficiently small compared to the region in which the first order Taylor expansion of the optimal value using \eqref{eq:47} is a reasonable approximation.
The integral \eqref{eq:50} can be evaluated as
\begin{equation}
  \label{eq:52}
  \operatorname{Var}\left[f^{\mathrm{opt}}\right] = \sum_{k=1}^K\int_{\mathbb{R}^T} \of{\frac{\partial f^{s,\mathrm{opt}}}{\partial \chi_k}(\omega-\chi_k)}^2\pi_k\mathcal{N}(\omega|\chi_k,\sigma_k)d\omega
\end{equation}
%Using the definition of the Gaussian, the recursive application of
%\begin{equation}
%  \label{eq:53}
%\int_{\mathbb{R}^n}(ax+b)^{tr}(ax+b)|2\pi\sigma|^{-1/2}\exp\of{-\frac{1}{2}(x-\mu)^{tr}\sigma^{-1}(x-\mu)} dx = b^{tr}b + a^2|\sigma|
%\end{equation}
which leads to the simple term
\begin{equation}
  \label{eq:54}
  \operatorname{Var}\left[f^{\mathrm{opt}}\right] = \sum_{k=1}^K \left(\frac{\partial f^{s,\mathrm{opt}}}{\partial\chi_k} \right)\left(\frac{\partial f^{s,\mathrm{opt}}}{\partial\chi_k} \right)^{tr}\pi_k|\sigma_k| = \sum_{t=1}^T\sum_{k=1}^K\of{\frac{\partial f^{s,\mathrm{opt}}}{\partial \chi_{k}^t}}^2 \pi_k|\sigma_k|.
\end{equation}
To compute the sensitivities, we will make use of the stage decomposition \eqref{eq:48} of the objective.
\begin{equation}
  \label{eq:51}
  \frac{\partial f^{\mathrm{opt}}}{\partial \nu_n} \overset{\eqref{eq:48}}{=} \sum_{(k,t)\in n^{-1}(n)}\pi_k\frac{\partial f_{t}^{s,\mathrm{opt}}}{\partial \chi_{k}^t}
\end{equation}
Since the variables $\xi_{k}^t$ are identical, if they are from the set $n^{-1}$, we can conclude that
\begin{equation}
  \label{eq:56}
  \frac{\partial f_t^{s,\mathrm{opt}}}{\partial \chi_{k}^t} = \left(\frac{\partial f^{\mathrm{opt}}}{\partial \nu_{n(k,t)}}\right) \left/ \left(\sum_{(j,t)\in n^{-1}(n)} \pi_j\right)\right.
\end{equation}
With this, the variance can be directly computed from \eqref{eq:54}.
This allows for an estimate of the reliability of the solution to the stochastic program.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "da"
%%% End:
