\section{Conclusion and outlook}
The discretization of stochastic processes is an important step in the solution of stochastic programs. In this thesis, three advances for the computation of this discretization are presented.

First, a rigorous MILP/NLP model that characterizes the optimal tree solution was developed. The model is the combination of a selection problem whose objective is given by the linear transportation problem of the Kantorovich distance. State-of-the-Art mixed integer solvers were used to solve this model. While it is too large to be solved for any practical problem, it serves as a benchmark for heuristic algorithms.

Second, using properties of the MILP characterization, the K-Means-for-trees algorithm has been derived. This heuristic algorithm is able to compute tree structures orders of magnitude faster than the MILP algorithm. Using the benchmark algorithms, the heuristic solutions have been shown to be near-optimal. This makes it possible to handle much larger data sets. The K-Means-for-trees algorithm was then translated into the more general framework of the Expectation Maximization algorithms. As an example, the tree generation was derived for a Gaussian Mixture model. It has been shown that the Gaussian Mixture model tree corresponds to a function space approximation of the stochastic process. Since the space in which the stochastic process is approximated acts as an implicit bias on the structure of the underlying probability distribution, in many cases where domain knowledge about the distribution is available, a superior approximation can be found.

Finally, it has been shown how an approximation of the discretization error in the objective of the stochastic program can be computed using a tree generated with the Expectation Maximization algorithm. This approximation relates the error in the approximation of the tree with the error in the solution of the stochastic program and assists in evaluating the error between the actual infinite-dimensional problem and its discretization.

In this thesis, for the first time to the authors knowledge the relation between the tree generation problem for stochastic programming and the Expectation Maximization algorithm has been shown. Future research should elaborate on some of the of the problems that were intentionally not covered in this thesis. Most importantly, it remains to be proven that the approach of fixing the filtration tree at the beginning of the algorithm is acceptable. On the practical side, the general iteration scheme of Expectation Maximization for trees should be applied a broader set probabilistic models tailored to the specific applications. Log-Normal base distributions are, for example, a promising approach to achieve a higher accuracy for the important class of processes describing prices.
\paragraph{Acknowledgments}
The author would like to acknowledge several software contributions that were helpful for implementing the algorithms of this paper: Gurobi, made available by Gurobi Inc. through Gurobi Optimization. The Gurobi Matlab interface, written by Wotao Yin. The CLP Matlab interface written by Johann L\"{o}fberg. The Kmeans++ algorithm implemented for Matlab by Laurent Sorber. The Kmedoids algorithm implemented in Matlab by Benjamin Sapp.
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "da"
%%% End:
