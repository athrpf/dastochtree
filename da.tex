\documentclass[a4paper, 12pt] {article}
%\usepackage[pd,col,3d,turt]{mgtex_23}
\usepackage{etex}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{pgfplots}
\usepackage[inline]{showlabels}

\newtheoremstyle{example}
     {\topsep}%      Space above
     {\topsep}%      Space below
     {}%         Body font
     {}%         Indent amount (empty = no indent, \parindent = para indent)
     {\bfseries}% Thm head font
     {}%        Punctuation after thm head
     {}%     Space after thm head: " " = normal interword space;
           %       \newline = linebreak
     {\underline{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}}%         Thm head spec (can be left empty, meaning `normal')
%     \theoremstyle{example}
     \newtheorem*{example}{Example}
\newtheorem*{Note}{Note}
\newtheorem{thm}{Theorem}
\begin{document}
\listoftodos
\newpage
\title{Advanced Methods in Stochastic Programming}
\author{Hans Pirnay}
\maketitle
\tableofcontents
\newpage
\section{Introduction}
\todo{write an introduction}
\section{Stochastic Programming Theory}
Consider the following real world problem:

An operator in charge of a pumped hydro plant at any given time wants to make the optimal decision wether to pump water up into his reservoir with electrical power purchased at current spot market prices, do nothing, or release water from the reservoir and sell it at the spot market. 

For an introduction into the general theory see \cite{Birge1997}. For a more recent overview on optimization under uncertainty, see \cite{Sahinidis2004} and the references therein.
% here, explain the following terms:
% - value of the stochastic solution
% - perfect foresight
% - here and now / wait and see
\section{Scenario tree generation}
The generation of the scenario tree is a key step in the solution of stochastic programming problems.

In this section we will present several ways to generate said scenario trees. The section is organized as follows. First, a short introduction to the basic mathematical theory will be given that is necessary to follow the derivations of the algorithms. Then, the state of the art in solving this problem is summarized. Finally, several new algorithms are derived. These new algorithms will differ from the ones proposed in the literature in that they attempt to solve the original problem to full optimality. The section will close with a discussion and comparision of the results of the presented algorithms. 
\subsection{Mathematical Foundations}
This section serves as a short introduction to the mathematical concepts used throughout this paper. The reader should be familiar with the most basic concepts of probability theory and measure theory, such as probability spaces, sigma algebras, filtrations and measures.

This section is organized as follows. First, the above description of a real world problem is tranlated into the context and notation of probability theory. Then the resulting abstract mathematical description is transformed into a numerically tangible formulation.
\subsubsection{Notation}
We will only deal with stochastic processes with discrete time steps. These time steps will be indexed from 1 to a final time T. For a stochastic process $\xi$ with a continuous distribution, $\xi^t$ denotes the mapping from events to outcomes at timestep $t$. For discrete stochastic processes, index sets for the events, typically denoted by $I$ and $J$, are introduced. For convenience, the stochastic process and its index set are used interchangably. The outcome of an event $i\in I$ of a discrete stochastic process $\xi$ at timestep $t$ is denoted by $\xi_i^t$. If the timestep is omitted, this denotes the full scenario $i$, that is the vector of the values $\xi_i^t$ sorted by their times $t$.
The vector $e$ denotes a vector of ones in each dimension. The dimension will be chosen in accordance with the context. 
\subsubsection{Stochastic Programming and Scenario Trees}
Suppose a problem given in the following generic nonlinear multi-stage stochastic structure. 
\begin{eqnarray}
  \label{eq:genericSP}
  \min &&\mathbb{E}\left[\sum_{t=1}^Tf_t(\xi_t, x_t)\right]\\
  &\mathrm{s.t.}& h_t(\xi_t, x_t, x_{t-1}) = 0\\
  &&x_t \in X_t\\
  &&x_t \, \mathrm{is}\,\mathcal{F}_t \mathrm{-measurable} \label{eqn:measurability-constraint}
\end{eqnarray}
In this problem, the stochastic process $\xi$ can be considered as a mapping $\xi:\Omega\rightarrow \mathbb{R}^{d\times T}$, where $\Omega$ is the sample space of the underlying probability space $(\Omega, \mathcal{F}, \mathbb{P})$. The solution is a mapping $x:\Omega\rightarrow \mathbb{R}^{n\times T}$.
 
The difficulty in multi-stage stochastic programming problems arises from the measurability constraint (\ref{eqn:measurability-constraint}). This constraint is specific to multi-stage stochastic programs.
This additional constraint leads to a much increased computational complexity of multi-stage programs over two-stage programs \cite{Shapiro2005,Shapiro2008}.
%
\par{Measurability Constraints} 
mean that the condition
\begin{equation}
  \label{eq:mathematical-NAC}
  \omega_1,\omega_2\in \Omega \, : \, \xi_\tau(\omega_1) = \xi_\tau(\omega_2)\,\forall \tau\leq t\,\Rightarrow \, x_t(\omega_1) = x_t(\omega_2) 
\end{equation}
has to hold for all time steps $t$, and all pairs of events $(\omega_1,\omega_2)$. 
The decision variables $x_t$ must only depend on the events of the past up until the present $\xi_1$, ..., $\xi_t$, and explicitly \textbf{not} on $\xi_{t+1},\, ...,\,\xi_T$. 
This property is often called \textit{non-anticipativity of the solution} $x$. 
Therefore, these equations model the uncertainty of the future.
%
\paragraph{Discretization}
%
To transform the stochastic programming problem into a NLP/LP, the infinite dimensional stochastic process $\xi$ must be discretized into a finite stochastic process $\hat{\xi}$. If certain precautions are taken, it is possible to show (at least for linear stochastic programs) that the solution to the stochastic problem is stable under this discretization operation, meaning that there exists a factor $L$ such that the distance between the solution to the discretized problem $\hat{x}$ and the solution to the original problem $x$ are Lipschitz-continuous with respect to the discretization error:
\begin{equation}
  D(x , \hat{x}) \leq L\cdot D(\xi,\hat{\xi})
\end{equation}
where $D$ is an appropriate norm. See \cite{Heitsch2009} for the proof and further details.

The discretization will be carried out in two steps. 
First, the infinite dimensional stochastic process $\xi$ is approximated by a set of scenarios sampled using Monte-Carlo techniques. 
This step is well known and will not be discussed here. 
To capture the essence of the the non-anticipativity constraints, in a second step the scenarios $I$ are recombined into a tree structure. 
Tree structures introduce the non-anticipativity constraints in intuitive way (see figure \ref{fig:NAC-and-trees}.
\begin{figure}
  \centering
  \missingfigure{NAC and Trees}
  \caption{Non-anticipativity constraints are automatically satisfied in tree-structures}
  \label{fig:NAC-and-trees}
\end{figure}
The parent node holds the information of the past, while each node is branched into multiple possible future states. 
It is, in fact, insufficient to represent the stochastic process as a set of non-interdependent scenarios with a common root. 
As this is a common misconception, for illustration purposes we will give the following
\begin{example}[Necessity of tree structure]
  \label{ex:tree-necessity}
%\hangindent=1cm
Consider a game of coin tosses, plaied by a player against a bank. 
The player will play the game for three consecutive coin tosses. 
He has three Euros that he all wants to bet. 
Predicting the outcome of the toss correctly will return $\frac{13}{6}$ the stakes $x$ in the first round, $2\cdot x$ in the second round, and only $1.5\cdot x$ in the final round. 
Otherwise the stakes will be lost. 
Earnings cannot be used to bet in the game.

The player optimizes the expected return.
Without the need for a calculator we can say that the optimal strategy is to bet everything in the first round, yielding a total expected return of $1.75$.

Consider now a transcription of the problem as an LP, using all possible scenarios (not in tree form). For three consecutive coin tosses, there are eight different scenarios. The player wants to use this LP to help him decide, how much he is supposed to bet in the first round. The corresponding LP is
\begin{align*}
  \min\limits_x &\; \sum_{s\in S}\sum_{t=1}^3p_sx_s^t\cdot c_s^t\\
  \text{s.t.} &\; \sum_{t=1}^3x_s^t = 3 \;\forall\, s\in S\\
  & \; x_s^1 = x_0^1\;\forall s\in S
\end{align*}
where $S$ is the set of scenarios, $c_s^t$ is the outcome of the game (return per Euro for win, $0$ for loss), $x_s^t$ is the amount that the player is willing to bet at timestep $t$ in scenario $s$. The first equation ensures that the player spends exactly the amount he has at his disposal. The second equation fixes the players decision in the first round.

The results, summarized in table \ref{tab:coin-toss-results}, show that the solution of the LP does not match the optimal solution, even though the stochasticity was seemingly taken into account. 
The flaw in the above formulation is that the non-anticipativity constraints are missing for stage 2. 
The algorithm believes to know what will happen, as soon as the first value is revealed. 
The game, as it is modeled in the LP is as follows: 
``Make one decision with stochastic outcome, then make two decisions with perfect foresight''. 
In this situation, the algorithm will correctly decide to wait and spend its money only on bets that it knows the result of beforehand.
\begin{table}
  \small\centering
  \begin{tabular}{lcccccccc}
    \hline 
    Scenario&1&2&3&4&5&6&7&8\\\hline\hline
    Result&WWW&WWL&WLW&WLL&LWW&LWL&LLW&LLL\\
    Opt. Solution&300&300&300&300&300&300&300&300\\
    LP Solution&030&030&003&003&030&030&003&003\\\hline
  \end{tabular}
  \vspace*{0.5cm}\\
  \begin{tabular}{lcc}
    \hline
    &Opt. Solution&LP Solution\\\hline\hline
    Opt. Value (computed)&1.75&1.875\\
    Opt. Value (real)&1.75&1.5\\
    \hline
  \end{tabular}
  \caption{Results of the coin-toss example}
  \label{tab:coin-toss-results}
\end{table}
\end{example}

The above example illustrates the need for tree structured stochastic processes. The construction of this tree structure will be the main point of the following paper. Before we can start constructing these scenario trees, we need to define a measure for the quality of a given approximation $\hat{\xi}$ to a stochastic process $\xi$. The goal is to define a distance function 
\begin{equation}
  \label{eq:distance-function-intro}
  D:\mathcal{S} \times \mathcal{S} \rightarrow \mathbb{R}_+,\;(\xi, \hat{\xi})\mapsto D(\xi, \hat{\xi})
\end{equation}
where $\mathcal{S}$ is the space of stochastic processes which remains to be defined. The distance function should satisfy at least the definition of a metric. Since the choices for $\mathcal{S}$ and $D$ are by no means obvious, they are discussed in the following section.
\subsubsection{Challenges in Measuring Stochastic Processes}
The purpose of this section is to derive a distance function which will serve as a means to evaluate the quality of a scenario tree approximation to a stochastic process. We will try to avoid as much of the theory and stick to the absolutely necessary. As we are dealing mainly with the numerics of this problem, the theoretical discussion is carried out with a focus on comprehensibility. For details see \cite{Heitsch2010} and the references therein.

Stochastic processes are very versatile and complex objects. Depending on the problem at hand, different interpretations may be useful. 

In general, a stochastic process (as stated above, we will focus exclusively on time-discrete stochastic processes) is a composition of five objects. The first three objects compose a probability space: a sample space $\Omega$, a filtration (family of $\sigma$-algebras) $\left\{\mathcal{F}_t\right\}_{t=1}^T$, and a probability measure $\mathbb{P}$ which is a mapping from the set of distinguishable events at time $t$ to a probability:
\begin{equation}
  \label{eq:prob-measure-definition}
  \mathbb{P}_t : \mathcal{F}_t \rightarrow \left[0,1\right]. 
\end{equation}
The final two elements of the stochastic process are a set of values that the stochastic process can take, and a mapping $\zeta$ from the sample space to the value space. For our purposes, the value space will always be a subset of the Euclidean space $\mathbb{R}^n$.

These elements that make up the stochastic process can lead to different interpretations, by fixing four of these five and considering the space of stochastic processes in terms of the fifth object. One popular way to do this is to fix everything but the mapping $\zeta$ from the sample space to the value space. An example of this usage is the proof of the stability properties of multi-stage stochastic linear programs in (\cite{Heitsch2010}). The corresponding space is the space of functions mapping $\Omega$ to $\mathbb{R}^n$. The exact space depends on the regularity of the functions one would like to consider. A natural choice is the space of p-integrable functions $L^p$ since there is no reason for the assumption of any kind of differentiability of $\zeta$ and $L^p$ has the nice feature of being a Banach Space, which allows for the definition of a norm
\begin{equation}
  \label{eq:Lp-norm}
  \left\Vert\xi\right\Vert_p = \sum_{t=1}^T\left(\int_{\omega\in \Omega}\left\Vert\zeta_t(\omega)\right\Vert^p d\mathbb{P}(\omega)\right)^{1/p}.
\end{equation}

The most common way to think about stochastic processes is, however, in terms of its probability measure $\mathbb{P}$.
For the purpose of this discussion, let $\Omega=\mathbb{R}^n$ and $\zeta=id$.
Each of the elements of the stochastic process except for the probability measure is fixed, and the stochastic process is considered in terms of the space of probability measures.
This interpretation will be followed throughout this paper.

The choice of the the space of probability measures as the underlying space for stochastic processes leads to much more difficulties when defining a metric. It is not feasible to regard two probability measures $\mathbb{P}_1$ and $\mathbb{P}_2$ as integrable functions and elements of $L^p$ for two reasons:
\begin{enumerate}
\item The metric between two stochastic processes $\xi_1$ and $\xi_2$ would be defined in terms of the $L^p$-norm between their probability measures $\mathbb{P}_1$ and $\mathbb{P}_2$:
  \begin{equation}
    \label{eq:prob-measure-metric-as-Lpnorm}
    D(\mathbb{P}_1,\mathbb{P}_2) := \left\Vert \mathbb{P}_1-\mathbb{P}_2\right\Vert = \sum_{t=1}^T\int_{\omega\in\Omega}\left\Vert \mathbb{P}_{1,t}(\omega)-\mathbb{P}_{2,t}(\omega)\right\Vert
  \end{equation}
  This does, however, not yield meaningful results. See figure \ref{fig:example-wrong-distance} for an example.
\item The definition of the above metric makes use of the pointwise difference of $\mathbb{P}_1$ and $\mathbb{P}_2$. 
  This difference is, however, not a meaningful construction, since the difference of two probability measure functions is itself \textbf{never} a probability measure.
\end{enumerate}
\begin{figure}
  \centering
  \missingfigure{two probability distributions moving closer together but same distance}
  \caption{Example of two different probability distances. 
    Even though the probability distributions on the right are closer than the ones on the left, they have the same distance in the $L^p$ norm.}
  \label{fig:example-wrong-distance}
\end{figure}
\subsubsection{The Kantorovich Distance}
In this section, we will present the Kantorovich Distance as a meaningful and usable metric for stochastic processes. For a gentle introduction to the Kantorovich Distance see \cite{Deng2009}.
%Note that stochastic processes can be represented as random variables by considering each trajectory as an event.

In the context of stability analysis of stochastic programming problems, it has been shown by \cite{Dupacova2003} that the Kantorovich functional
\begin{equation}
  \label{eq:define-infinitedim-kantorovich}
  \mu_c(\mathbb{P}, \mathbb{Q}) = \inf\left\{\int_{\Omega\times\Omega}c(\omega, \hat{\omega})\eta(d(\omega,\hat{\omega})),\, \eta\in\mathcal{M}\right\}
\end{equation}
represents a  ``natural and suitable''\cite{Dupacova2003} distance for measuring stochastic processes in the context of stochastic programming problems. 
Here, $\mathcal{M}(\mathbb{P, Q})$ is the space of Borel-measures with marginals $\mathbb{P}$ and $\mathbb{Q}$, meanging that $\eta$ is a probability measure on the space $\Omega\times\Omega$ with the properties
\begin{equation}
  \label{eq:define-borel-measures}
  \int_{B\times \Omega} \eta(d(\omega,\hat{\omega})) = \mathbb{P}(B),\;   \int_{\Omega\times B} \eta(d(\omega,\hat{\omega})) = \mathbb{Q}(B),\; B \in \mathcal{B}
\end{equation}
where $\mathcal{B}$ is the Borel-$\sigma$-field with respect to $\Omega$.
The distance $c$ is a mapping with certain properties, which are specified in \cite{Dupacova2003}.
For the purposes of this paper, it suffices to say that any vector norm satisfies these conditions.

\begin{Note}
In this section, the Kantorovich Metric as applied to the stochastic process as a random variable has been proposed for measuring stochastic processes in stochastic programming.
In the derivation of the stability of multistage stochastic processes (\cite{Heitsch2010}), the $L^p$ norm (see eqn. \ref{eq:Lp-norm} in conjunction with a ``filtration distance'' was used.
Recently, it has been shown that previous heuristics (for example those in \cite{Dupacova2003}) that disregarded the filtration distance fail to uphold stability of the problem (\cite{Heitsch2009a}).
These methods, as opposed to the one presented here, had obvious disregard for the filtration, because they were based on sums over all stages with stage-dependent terms.
The methods for scenario tree construction presented in this paper ensure the correctness of the filtration through the tree structure, that is postulated beforehand.
\end{Note}
\paragraph{Kantorovich Distance for finite stochastic processes}
The main advantage of the Kantorovich Distance is its simple and intuitive representation for finite dimensional stochastic processes.
Consider now two finite dimensinal stochastic processes with sets of scenarios $I$ and $J$ respectively.
Translating the infinite dimensional representation of the metric \ref{eq:define-infinitedim-kantorovich} into this framework is straightforward.
The integral over the combination of all possible events of the two stochastic processes is replaced by a sum over this finite set.
The distance $c$ stays the same.
The measure $\eta$ in its finite equivalent is a matrix of weights satisfying the same conditions of the marginal probabilities.
These conditions are
\begin{align}
  \label{eq:finitedim-marginals-eta}
  \sum_{i\in I} \eta_{ij} &= q_j\\
  \sum_{j\in J} \eta_{ij} &= p_i.
\end{align}
Since in the finite case, the argument of the infimum is a finite sum over variables in a compact domain, the infimum can be replaced by the minimum. Combining these pieces into the full formulation yields
\begin{equation}
  \label{eq:define-finitedim-Kantorovich}
  \mu_c(\mathbb{P}, \mathbb{Q}) = \min\left\{\sum_{i\in I}\sum_{j\in J}c(\xi_i,\hat{\xi}_j)\cdot \eta_{ij},\; \sum_{i\in I}\eta_{ij}=q_j,\;\sum_{j\in J}\eta_{ij}=p_i\right\}.
\end{equation}
This formulation is a linear program, more specifically a minimum cost flow problem. This means that solutions are readily available even for large problems using state of the art LP solvers. The above minimum cost problem will be at the heart of the derivations of the following sections, as it will serve as the measure for the quality of approximations to infinite dimensional stochastic processes.
% TODO: derive the min flow problem formulation. Explain here that the \sum_{j\in J}q_j =1 equation is redundant. Mention LICQ
%\subsubsection{The Kantorovich Distance in a function space approximation interpretation}
\subsection{State of the Art in Scenario Tree Approximations}
Recent research in the field of generating scenario trees for stochastic programs has focused on the areas of theoretical derivation of stability results and heuristics for generating scenario trees from stochastic processes. 

Despite the fact that these problems appear naturally in many contexts like manufacturing and financial services, the area of multistage stochastic programs has seen far less attention than its two-stage counterpart. Though this interes has grown over the recent years, researchers remain reluctant, mainly due to the computational complexity of the problems, even going so far as to call these problems generally intractable \cite{Shapiro2005}.



In the area of stability results, significant progress was achieved. Stability of mu
\subsubsection{The work of Roemisch}
\subsection{Scenario Tree Generation: Problem Definition}
In this section we will briefly state the problem definition for which solutions will be proposed below.

Let $\mathcal{P}$ denote a stochastic programming problem with the underlying infinite dimensional stochastic process $\xi_{orig}$.
Assume that it is possible to sample from this stochastic process.
Using Monte-Carlo method a set of pairs of scenarios and corresponding probabilities $(\xi_i,p_i)$ can be sampled.
This set of scenarios itself can be considered as a new stochastic process $\xi$ with a discrete probability distribution.
For a sufficiently large number of samples, we have $D_K(\xi,\xi_{\mathrm{orig}})<\epsilon$ with high probability.
The task is to find a discrete stochastic process defined as the discrete stochastic process $\hat{\xi}$ that satisfies the following properties:
\begin{enumerate}
\item $\hat{\xi}$ minimizes the Kantorovich Distance $D_K(\xi,\hat{\xi})$ defined in (\ref{eq:define-infinitedim-kantorovich}).
\item $\hat{\xi}$ exibits a given tree structure (see figure \ref{fig:generic-tree-structure}.  We assume the general tree structure - that is the number and order of the nodes - to be fixed beforehand. 
\end{enumerate}
In the literature, especially in \cite{Heitsch2009} and related papers (\cite{Dupacova2003,Heitsch2003,Heitsch2009a,Heitsch2010}), not the tree structure is fixed beforehand, but instead a given tree is reduced by deleting nodes until a maximum distance $D_K(\xi,\hat{\xi})$ is reached.
We view the approach of fixing the tree structure beforehand is a more sensible assumption for the following reasons.
\begin{itemize}
\item a recursive tree structure makes sense in most applications, as the structure of the decisions to be made is recursive as well. Consider the operator of the pumped-hydro plant mentioned above. He faces the same decision at each time step, and his measure of uncertainty for any event happening a certain amount of time into the future is independent of the current situation.
\item the number of scenarios is not determined by the error one will allow but is is the other way around: The error is determined by the maximum number of scenarios one can handle computationally. This number of scenarios will be known beforehand. As the correlation of using more scenarios and lowering the approximation error is strictly monotonic, there is no reason to use less scenarios than one could handle.
\item The value of the Kantorovich distance does not necessarily have a real-world interpretation. Therefore it might be difficult to postulate a meaningful bound.
\item For the method described in the papers cited above, it is not obvious how the original tree which is then reduced was constructed in the first place.
\end{itemize}

In an abstract notation, the problem of constructing a scenario tree from a given stochastic process can be expressed as the following optimization problem:
\begin{equation}
  \label{eq:symbolic-optimization-problem}
  \min_{\hat{\xi}}\left\{D_K(\hat{\xi}, \xi)\left|\hat{\xi} \in \mathcal{T}\right.\right\}
\end{equation}
where $\mathcal{T}$ is the set of discrete stochastic processes that exhibit the demanded tree structure.
This tree structure is represented by a set of nodes which each have one father but several children.
The probabilities of the nodes of each tree must be such that the probability of a node equals the sum of the probabilities of all children of this node. 

A tree structure can be defined by the number of stages $n_s$ and the number of children (branches) $n_c$ to each node. For a tree defined in that way, the number of scenarios in this tree (which is the same as the number of leaf nodes) is
\begin{equation}
  \label{eq:number-of-leaf-nodes}
  n_L = n_c^{n_s-1}.
\end{equation}
The number of nodes in the tree can be computed with the formula for the geometric series
\begin{equation}
  \label{eq:number-of-nodes}
  n_N = \frac{1-n_c^{n_s}}{1-n_c}
\end{equation}
Since both $\xi$ and $\hat{\xi}$ are discrete stochastic processes, the Kantorovich Distance is equivalent to the minimum cost flow problem described above (\ref{eq:define-finitedim-Kantorovich}).
Including this definition yields
\begin{equation}
  \label{eq:symbolic-optimization-with-minflow}
  \min_{\hat{\xi}}\left\{\min_{\eta,q}\left\{\sum_{i\in I}\sum_{j\in J}\eta_{ij}c(\hat{\xi}, \xi)\left|\sum_{i\in I}\eta_{ij}=q_j,\;\sum_{j\in J}\eta_{ij}=p_i\right.\right\}\left|\hat{\xi} \in \mathcal{T}\right.\right\}
\end{equation}
The probability distribution $q$ of the stochastic process $\hat{\xi}$ is not postulated beforehand, but must be determined as part of the optimization problem. Note that even though $q$ is not explicitly defined as a probability distribution, the equation
\begin{equation}
  \label{eq:q-schliessbedingung}
  \sum_{j\in J} q_j = 1
\end{equation}
is not necessary, and would in fact be redundant. The probabilities $p_i$ are known to form a probability distribution, therefore it holds that
\begin{equation}
  \label{eq:proof-sum-q-redundant}
  1 = \sum_{i\in I}p_i = \sum_{i\in I}\left(\sum_{j\in J}\eta_{ij}\right)=\sum_{j\in J}\left(\sum_{i\in I}\eta_{ij}\right)=\sum_{j\in J} q_j.
\end{equation}
This is an important fact in formulating the optimization problems of the following sections, since introducing this equation would violate the constraint qualifications MFCQ and LICQ.
A constraint qualification is crucial if KKT based optimization algorithms such as interior point methods are to be employed for the solution of the optimization problem \cite{Jongen2004}.

The inner minimization can, of course, be combined with the outer minimization to yield the final formulation
\begin{equation}
  \label{eq:symbolic-optimization-with-minflow2}
  \min_{\hat{\xi},\eta,q}\left\{\sum_{i\in I}\sum_{j\in J}\eta_{ij}c(\hat{\xi}, \xi)\left|\sum_{i\in I}\eta_{ij}=q_j,\;\sum_{j\in J}\eta_{ij}=p_i,\;\hat{\xi} \in \mathcal{T}\right.\right\}
\end{equation}
The following sections discuss two different Solutions to (\ref{eq:symbolic-optimization-with-minflow2}). These two approaches differ in the way the set $\mathcal{T}$ is defined.
\input{kantorovich_theorems}
\input{milpformulation}
\subsection{NLP Formulation}
In this section, we will present a novel approach to approximating scenario trees. While the previous section was concerned with generating scenario trees for discrete underlying event spaces, continuous event spaces are very common. Obvious examples for stochastic processes with continuos state spaces are prices and demands.

In cases where intermediate values are sensible choices for scenarios, the algorithm described above will sacrifice two possibilities for improvement. First, notice that the general structure of the problem is still given by the same minimum flow problem (\ref{eq:symbolic-optimization-with-minflow2}). The difference to the MILP formulation is the set of feasible trees $\mathcal{T}$. If intermediate values for the states are acceptible, as opposed to only those that were part of the original scenario set, it trivially holds that
\begin{equation}
  \label{eq:T-D-subset-T-C}
  \mathcal{T}_D\subset \mathcal{T}_C,
\end{equation}
where $\mathcal{T}_D$ is the set of feasible trees in the discrete case, and $\mathcal{T}_C$ is the set of feasible trees in the continuous case. It is therefore obvious, that the objective function for the continuous case must be equal to or lower than that of the discrete case. This means, that the continuous formulation allows for a tighter approximation.

The second advantage is the fact, that the continuous problem can be modeled with much fewer variables. This allows us to solve all stages at once, giving us the oportunity to overcome the suboptimality introduced by the stage-wise solution.

Something one does not have to worry about: Given a stochastic distribution that is composed of two hills, some might be worried that the algorithm might create intermediate values. This is, however, not a concern, since an intermediate solution will lead to a higher Kantorovich Distance.

In the following seciton, we will present a derivation of the model used to construct the trees with feasible intermediate values. Along the same lines as the previous section, we will then discuss the computational results. We will discover that the solution of the model with traditional NLP solvers is impossible. The reasons for their failure will be discussed, and a remedy in the form of genetic optimization will be proposed.
\subsubsection{Derivation of the NLP}
We start with the set of original scenarios $I$ with values $\xi_i^t$ and probabilities $p_i$. As before, we assume the structure of the tree to be given. We introduce additional sets $J$ for the scenarios of the trees, $T$ for the set of time steps, and $N$ for the nodes of the tree. The mapping $n:J\times T\rightarrow N$ is defined as the one-to-one mapping from scenarios and time steps to nodes (see figure \ref{fig:abstract-scenario-tree-for-NLP}). In the MILP model, there was no necessity to differentiate between the scenarios and the nodes, because the problem was solved stage by stage, so there was a one-to-one mapping between these two. Here, however, one scenario will always contain multiple nodes, and all nodes except for the leaf nodes will belong to more than one scenario.

The variables in this model are, in general the same as in the abstract model (\ref{eq:symbolic-optimization-with-minflow2}) and the MILP model, but there are some differences due to the structure. The probabilities $q$ of the nodes will not be defined directly over the set of nodes, but the set of scenarios. Given the probabilities of the scenarios, the probability of a node can easily be derived by adding up the probabilities of all scenarios this node belongs to. This definition has the advantage, that the probabilistic relations between nodes are automatically satisfied - for example the condition that the probability of every node has to be equal to the sum of probabilities of its children.  Similarly, the (discretized) measure $\eta_{ij}$ specifies the flow between an original scenario $i \in I$ and a scenario of a tree $j \in J$.

Since the exact values of the tree's nodes are not known beforehand, the distance between the nodes and the original scenarios are not known either. The distance is usually a nonlinear, non-differentiable function. For now, we will replace it with an additional variable $c$, and deal with modelling it farther below. Note that while we think of $c$ as a mapping on $I\times N$, this is - due to the one-to-one mapping induced by $n$ - the same as $I\times J\times T$, which is more convenient.

The objective function is again the Kantorovich functional adopted to the particular tree structure.
\begin{equation}
  \label{eq:NLP-derivation-objective}
  \sum_{i\in I}\sum_{j\in J}\sum_{t\in T} \eta_{ij}\cdot c_{ijt}
\end{equation}
As always, the following properties of the marginal distributions of the measure $\eta$ have to hold:
\begin{eqnarray}
  \label{eq:eta-nlp-marginal-q}
  \sum_{i\in I}\eta_{ij} &=& q_j \;\forall\, j\in J\\
  \label{eq:eta-nlp-marginal-p}
  \sum_{j\in J}\eta_{ij} &=& p_i \;\forall\, i\in I
\end{eqnarray}
As was shown above, these equations already ensure, that the probabilities of the tree's scenarios sum to one. Introducing the bounds
\begin{equation}
  \label{eq:bounds-nlp-q-eta}
  0 \leq \eta,\; 0\leq q
\end{equation}
completes the problem except for the distances $c_{ijt}$.

To model the distances, a metric must be selected. The choice of the best metric depends on the problem belongs to the domain of modeling rather than algorithmic questions that we address here. In the following, we will show how to model the distance $c_{ijt}=c(\xi_i^t,\nu_j^t)$ in the most common norms $\Vert\cdot\Vert_1$, $\Vert\cdot\Vert_2$, and $\Vert\cdot\Vert_\infty$.
%
\paragraph{$\infty$-norm} The one norm is defined as the maximum over the absolute value of all components. 
\begin{equation}
  \label{eq:max-c-definition}
  c_{ijt} = \max\left\{\left|\xi_{id}^t-\nu_{n(j,t)d}\right|,\; d\in D\right\}
\end{equation}
where $D=\left\{1,\, ...,\,n\right\}$ is the set of dimensional indices of the stochastic process. The absolute value can be replaced by the maximum expression
\begin{equation}
  \label{eq:abs-is-a-max}
  \left|\xi_{ik}^t-\nu_{n(j,t)d}\right| = \max\left\{\xi_{ik}^t-\nu_{n(j,t)d},\, \nu_{n(j,t)d}-\xi_{ik}^t\right\}.
\end{equation}
The maximum operator inside the maximum can, of course, be omitted. What is left is the maximum of a finite number of terms. This maximum can be expressed inside the NLP by adding $ |D| \cdot |I|\cdot |J|\cdot |T|$ pairs of inequality constraints of the form
\begin{eqnarray}
  \label{eq:c-as-inftynorm}
  c_{ijt} &\geq& \xi_{id}^t - \nu_{n(j,t)d},\\
  c_{ijt} &\geq& \nu_{n(j,t)d} - \xi_{id}^t.  
\end{eqnarray}
\paragraph{$\mathbf{1}$-norm} Modeling the $1$-norm in the language of inequalities is somewhat more involved, but works basically the same way as the $\infty$-norm. Additional variables $a_{dijt}$ have to be introduced to express the absolute values of the differences in each dimension of each pair, defined by
\begin{eqnarray}
  \label{eq:c-as-1norm-def-a}
  a_{ijtd} &\geq& \left| \xi_{id}^t - \nu_{n(j,t)d}\right| \\
  a_{ijtd} &\geq& \left|  \nu_{n(j,t)d} - \xi_{id}^t\right|
\end{eqnarray}
The distances are then defined by
\begin{equation}
  \label{eq:c-as-1norm}
  c_{ijt} = \sum_{d \in D} a_{ijtd}.
\end{equation}
\paragraph{$\mathbf{2}$-norm} The $2$-norm is much easier to model than the $1$-norm and the $\infty$-norm since the squaring makes the absolute values that give rise to the excessive use of additional inequality constraints. Instead, the distance variables can be defined by the simple equations
\begin{equation}
  \label{eq:c-as-2norm}
  \left(c_{ijt} \right)^2 = \sum_{d \in D}\left( \nu_{n(j,t)d} - \xi_{id}^t \right)^2
\end{equation}
For our purposes, the choice is arbitrary. For the common case of single-valued stochastic processes, the three definitions coincide. In our implementations, we will use the formulation of the $\infty$-norm, since it offers the smallest number of additional variables while at the same time being a linear equation. The $2$-norm has only half the number of equations, but is subject to problems in the scaling and introduces a nonlinearity in the constraints, which seems favourable to avoid.

The discussion closes the derivation of the NLP model for generating scenario trees. The following sections will be dedicated to solving this NLP. In the next esection, experiences with KKT-based solvers is discussed. The solution is found to be problematic, with convergence and local optimality issues. To solve the problem to full optimality, the use of global (heuristic) optimization techniques is explored in the following section.
\subsubsection{Structure of the Optimization Problem}
In this section, the optimization problem derived in the previous section will be restated in its entirety, and its structural properties will be discussed to the degree that it is useful for solving it.
\begin{eqnarray}
  \label{eq:full-nlp-restated-objecive}
  \min_{\eta, q, c}&&  \sum_{i\in I}\sum_{j\in J}\sum_{t\in T} \eta_{ij}\cdot c_{ijt}\\\label{eq:full-nlp-restated-q}
  \mathrm{s.t.}&&\sum_{i\in I}\eta_{ij} = q_j \;\forall\, j\in J\\
  \label{eq:full-nlp-restated-p}
  &&\sum_{j\in J}\eta_{ij} = p_i \;\forall\, i\in I\\
  \label{eq:full-nlp-restated-ineq1}
  &&c_{ijt} \geq \xi_{id}^t - \nu_{n(j,t)d},\\
  \label{eq:full-nlp-restated-ineq2}
  &&c_{ijt} \geq \nu_{n(j,t)d} - \xi_{id}^t\\ 
  &&0 \leq \eta,\; 0\leq q
\end{eqnarray}
The problem is a nonconvex, quadratic optimization problem. For the example of three time steps, the Hessian of the objective function has the structure
\begin{equation}
  \label{eq:structure-of-quadratic-hessian}
  Q = \left[\begin{array}{ccccc}
      0&0&0&I&0\\0&0&0&I&0\\0&0&0&I&0\\I&I&I&0&0\\0&0&0&0&0
    \end{array}\right]
\end{equation}
if the variable ordering is $(c,\eta, q)$. This matrix obviously is not positive definite. In fact, it can be found that the inertia of this matrix, meaning the triple of the numbers positive, negative, and zero variables, is
\begin{equation}
  \label{eq:inertia-of-hessian}
  \left(n, n, T\cdot n\right)
\end{equation}
where $n=|I||J|$, and $T$ is the number of stages (Proof very easy, maybe in appendix?). \cite{Pardalos1991} showed that quadratic optimization problems with at least one negative eigenvalue are $\mathcal{NP}$-hard.
\subsubsection{Solution with local (KKT)-NLP solvers}
In this section, we will discuss the applicability of general purpose KKT solvers to the problem stated above. There are several issues that make this problem particularly hard to solve for KKT based solvers. Among these problems are
\begin{description}
\item[Negative Curvature] KKT solvers use first and second derivatives to generate search directions. Note that KKT points are only stationary points, not guaranteed to be minima of the problem. In the situation of negative curvature, the local maximum is the attractor of Newton's method applied to the KKT-problem. Special care must be taken by the algorithm to reject search directions that lead to a KKT point that is a local maximizer for the objective function
\item[Local Minima] KKT points are an inherently local property of an optimization problem. The only way to find the global optimum is through extensive spatial search. This search can be conducted by applying a deterministic KKT solver to many different, randomly selected starting points.
\item[Large Scale] Due to the complications in modeling the distances $c$, and the fact that using a greater number of original scenarios will always increase the accuracy of the approximation, the solver has to deal with a very large number of variables - typically on the order of one million.
\end{description}
We used the interior point solver IPOPT (\cite{IpoptImplementation2006} as the KKT solver. IPOPT is specifically designed to deal with large-scale non-convex programming problems. It has provisions that ensure global convergence to a KKT point associated with a local minimum under very soft assumptions.

As is to be expected from the theoreticas analysis above, solving the problem from different starting points leads to very different local minima. Figure \ref{fig:different-local-minima-with-ipopt} shows some examples of local minima the algorithm found. In addition to the locality of the solutions, we experienced an unexpectedly slow convergence as the objective value of the iterates approached the optimal solution, with very large primal steps taken by the algorithm. 

The reason for the poor local convergence again lies in the structure of the problem. Consider the conditions derived for locally superlinear convergence of IPOPT given in \cite{Waechter2005}. There, it is stated that, in order to be able to achieve superlinear local convergence, the sufficient second order conditions (SSOC) must hold for the optimal point. 

One condition among these is that at the optimal point $\left(x^*,\lambda^*\right)$ the Hessian of the Lagrangean $\nabla_{xx}^2\mathcal{L}(x^*,\lambda^*)$ is positive definite when projected onto the nullspace of the constraint Jacobian matrix $\nabla c(x^*)^T$. In our specific case of a quadratic objective function with only linear constraints, the Hessian of the Lagrangean is the matrix $Q$ introduced in equation \ref{eq:structure-of-quadratic-hessian}. The constraints are stated in equations \ref{eq:full-nlp-restated-q} through \ref{eq:full-nlp-restated-ineq2}. IPOPT does not deal directly with inequality constraints, so slack variables $s_{1ijt},\,s_{2ijt} \geq 0$ are introduced and equations \ref{eq:full-nlp-restated-ineq1} and \ref{eq:full-nlp-restated-ineq2} restated as
\begin{eqnarray}
  \label{eq:nlp-ineq1-with-slacks}
  c_{ijt} +s_{1ijt}&=& \xi_{id}^t - \nu_{n(j,t)d},\\
  c_{ijt} +s_{2ijt} &=& \nu_{n(j,t)d} - \xi_{id}^t.
\end{eqnarray}
With this reformulation, the SSOC for the problem can be determined by simple algebra: There are $|J|+|I||J|+3*|I||J||T|$ variables in the problem. The number of positive eigenvalues in the Hessian of the Lagrangean is $|I||J|$ (correponds to ``$n$'' in eqn. \ref{eq:inertia-of-hessian}). All remaining $|J|+3*|I||J||T|$ have to be matched by constraints, because each constraint can only eliminate one direction. There are, however, only $2*|I||J||T|+|I|+|J|$ constraints, so that a total of $|I||J||T|-|I|$ variables with eigenvalues of zero remain unmatched. These directions correspond to the distances $c_{ijt}$ which are not acive, e.g. these indices $i,j$ for which $\eta_{ij}=0$. For these distances, the solution is not unique, since any arbitrary feasible value chosen for them will - all else being equal - yield the same value of the objective function. Due to the barrier term that is added to the objective function, those variables $c$ will be driven arbitrarily far away from their bound, which is the behaviour that was observed.
\begin{figure}
  \centering
  \includegraphics[scale=0.6]{sixgraphs}
\caption{Six solutions of the NLP using IPOPT, starting from random starting points. The distribution to be approximated are independent gaussian distributions. The optimal solution would consist of each node placing its child nodes at the }
  \label{fig:different-local-minima-with-ipopt}
\end{figure}

In conclusion it must be stated that this method did not seem sufficient to adress the problem. Due to the abundance of local optima, some kind of global search has to be conducted. However, due to the poor convergence property of the explored algorithm, the local searches are not fast enough to be able to cover the search space in a reasonable time. 

In the next section, global search approaches are discussed, that are able to overcome some of the difficulties we have faced here.
\subsubsection{Escaping local optima: Global Optimization}
\label{sec:nlp-formulation2}
Global optimization approaches can generally be assigned to one of two categories. The first of those two categories is the class of rigorous spatial branch and bound algorithms. These algorithms work on the idea of underestimating the given function on smaller subdomains by convex functions. The problem is solved for the convex underestimator, which yields a lower bound on the objective. This is conducted until the optimality gap is reasonably small. The second approach is to use stochastic optimization algorithms such as genetic optimization and swarm intelligence.

In this section, both of the above approaches will be applied to solve the NLP to full optimality. First, it will be shown that the spatial branch and bound algorithm can for this problem be expressed as an MILP, which makes the solution much more achievable. In the second part, the application of stochastic search routines to the NLP will be discussed.

\paragraph{The global NLP as an MILP} For the current problem, we can actually derive a much smaller search region. From the property proven above (\ref{to-corresponding-proof}), that
\begin{equation}
  \label{eq:restate-eta-either-p-or-0}
  \eta_{ij} = p_i \;\vee \; \eta_{ij}=0,
\end{equation}
we can transform this problem into a selection problem: Find the mapping from $|I|$ to $|J|$ such that the following optimal value of the linear optimization problem
\begin{eqnarray}
  \label{eq:global-NLP-as-MILP-inner-LP}
  &\min\limits_{c,\nu}&\sum_{i\in I}\sum_{j\in J}\eta_{ij}\sum_{t\in T}c_{ijt}\\
  &\mathrm{s.t.}&c_{ijt} \geq \xi_{id}^t - \nu_{n(j,t)d},\\
  &&c_{ijt} \geq \nu_{n(j,t)d} - \xi_{id}^t
\end{eqnarray}
The full problem can be expressed as an MILP using the \textbf{big-M}-formulation. The basic idea is not to use rule \ref{eq:restate-eta-either-p-or-0} directly, but instead set $\eta_{ij}=p_i$ and manipulate $c_{ijt}$ such that it is zero if the selection binary variable $z_{ij}$ is zero. A basic version of this is the model
\begin{eqnarray}
  \label{eq:NLP-as-MILP-full}
  &\min\limits_{z,c,\nu}&\sum_{i\in I}\sum_{j\in J}\sum_{t\in T}p_i\cdot c_{ijt}\\
  &\mathrm{s.t.}&c_{ijt}+ \nu_{n(j,t)d}+M(1-z_{ij} \geq \xi_{id}^t\\
  &&c_{ijt} -\nu_{n(j,t)d} +M(1-z_{ij}\geq - \xi_{id}^t\\
  &&0\leq c_{ijt} \leq M\cdot z_{ij}\\
  \label{eq:NLP-as-MILP-full-bounds}
  &&z_{ij}\in \left\{0,1\right\}
\end{eqnarray}
with a sufficiently large $M$. There are many ways this formulation could be improved. The big-M formulation might be replaced by a convex hull, and symmetry breaking constraints could be introduced, to just name two. However, with several hundred thousand binary variables,  this problem is way out of reach for today's computing technology to solve to full optimality.

Similar to the MILP selection problem in section \ref{sec:MILP-selection-problem}, the problem size can be significantly reduced by decomposing the problem along the time dimension. If only one stage is solved at the same time, the number of binary variables is reduced from $n_I\cdot n_c^{n_s}$ to $n_I\cdot n_c$, where $n_s$ is the number of stages of the problem, $n_c$ is the number of children per node, and $n_I$ is the number of original scenarios, that were selected to belong to the father node. Even though the smaller problem has to be solved much more often ($n_c^{n_s-1}$ times), this approach is at least remotely feasible. The problem to be solved at each stage is exactly the same as the one presented in equations \ref{eq:NLP-as-MILP-full} through \ref{eq:NLP-as-MILP-full-bounds}, with the set $I$ replaced by the subset of the original nodes that the current node's parent node was selected to cover, and the set $J$ encompasses only the nodes of the tree that are children of the current node.

The decomposition of the temporal dimension might, of course, lead to suboptimal solutions. It may well be, that pronounced suboptimalities introduced by this decomposition appear only in pathological examples. Nevertheless, it would be advantageous to have an algorithm, that could solve all stages simultaneosly. In the next section, we will show how stochastic optimization algorithm can achieve this goal.
\paragraph{Stochastic Search Algorithms}
Since the solution of the full scale MILP presented above is not yet a feasible alternative, we will apply a stochastic optimization algorithm to the problem. Stochastic optimization algorithms usually have the following common features;
\begin{itemize}
\item They rely solely on function values.
\item The search path is stochastic
\item They do not attempt to satisfy a descent property
\end{itemize}
As the algorithm only uses function values, and is in general unable to satisfy additional constraints, the search space will be reduced to the set of variables that are in fact free. These variables are the locations $\nu_{jd}^t$ of the tree nodes. The remainig variables that are necessary to evaluate the objective (e.g. the $\eta_{ij}$ will be computed implicitly. This means that the linear programming problem for the Kantorovich Distance has to be solved at each objective function iteration. The objective, as it is posed to the stochastic optimization solver, is
\begin{eqnarray}
  \label{eq:opt-problem-for-stochastic-solver}
  \min_{\nu}&&f(\nu)\\
  \mathrm{with}\; f(\nu)&=\min\limits_\eta &\sum_{i\in I}\sum_{j\in J}\sum_{t\in T}\eta_{ij}c(\nu_j^t,\xi_i^t)\\
  &\mathrm{s.t.}&\sum_{j\in J}\eta_{ij} = p_i\\
  &&0\leq \eta_{ij}
\end{eqnarray}
Note that that the equation
\[
\sum_{i \in I}\eta_{ij} = q_j
\]
is not necessary, since they do not impose any additional restriciton on the choice of $\eta_{ij}$ - they only serve to define the probabilities $q_j$. (The constraint $q_j\geq 0$ can be deduced from $\eta_{ij}\geq 0$.) These probabilities are, however, unnecessary for the evaluation of the objective function. Since $f(\nu)$ will have to be evaluated very often, it is advisable to keep the internal optimization problem as small as possible.

There are several well-studied, stochastic optimization
algorithms available. We chose Particle Swarm Optimization for the
following reasons:
\begin{description}
  \item[swarm] The local optima are located in distinctly different places of the search space. Algorithms that imitate swarm behaviour intuitively act like multiple starting points in this situation
    \item[locality of the search] At every iteration, the inner linear optimization problem must be solved. If the variables $\nu$ are chosen as perturbations of the previous iterate, the solution of the inner LP is likely not to change.
\end{description}

\subsection{Results}
In this section, the results of the different algorithms for tree construction will be compared. To achieve a good basis for comparison, we will first apply all algorithms to the same simple problem to which the optimal solution is known. 
\subsubsection{Results fl}
\subsection{Conclusion}
\section {example problems}
\subsection{pumped hydro}
\subsubsection {Introduction}
The recent developments have shown an emergence of renewable energy sources such as wind and solar. Besides having no carbon emissions, these two forms of power generation share the issue of unpredictable availability. Due to their dependence on the weather, electricity is not necessarily available when needed. The only tool for storing electricity that is currently available are pumped hydro storage plants.

With the liberalization of the electricity markets and the introduction of electricity exchanges (EEX) in Germany and elsewhere, the valuation of power generation assets has been simplified significantly, since all power generating assets (small amounts) can be valued as put options against the electricity spot price.

Because of the stochastic nature of the underlying physics that lead to the formation of the electricity price, 
\subsubsection{Description}
Consider a hydro pumped storage plant. Figure x illustrates the problem the operator has to solve.

The pumped storage hydro power plant is determined by three variables – The flow x that is being pumped into the reservoir, the flow y that is leaving the reservoir, and the reservoir's current level P.  The optimization problem to be solved is the maximization of the expected profit. 
\subsubsection{Mathematical Formulation and analysis}
Given a stochastic process $\xi$, the problem in its complete form is
\begin{equation}
\begin{array}{cc}
\displaystyle \min_{x,y,P} &\displaystyle\int_0^T \int_{\omega \in\Omega}\xi(\omega,t)\left[x(\omega,t)-y(\omega,t)\right]p(\omega,t)d\omega dt\\
s.t.& \displaystyle\frac{\partial P}{\partial t} = \eta x -y\\
&0\leq x \leq x_{\mathrm{max}}\\
&0\leq y \leq y_{\mathrm{max}}\\
&0\leq P \leq P_{\mathrm{max}}\\
&\displaystyle x(\omega, t)\:\mathrm{ is }\:\mathcal{F}_t(\xi)\:\mathrm{ - measurable.}
\end{array}
\end{equation}
The single part of this problem that is causing the most problems is the measurability constraint. 
\newpage
\bibliographystyle{plain}
\bibliography{da}
\end{document}
